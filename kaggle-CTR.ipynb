{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4341694-8881-419c-a761-7f1951f7026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据读取成功！形状为: (2000000, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>f54016b9</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>07b5194c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>c5c50484</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>9727dd16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>b04e4670</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>60f6221e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>43f13e8b</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>731c3655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8efede7f</td>\n",
       "      <td>3412118d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>ad3062eb</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1e88c74f</td>\n",
       "      <td>74ef3502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6b3a5ca6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>9117a34a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1e88c74f</td>\n",
       "      <td>26b3c7a7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21c9516a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>b34f3128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   I1   I2    I3    I4      I5    I6    I7   I8     I9  ...       C17  \\\n",
       "0      0  1.0    1   5.0   0.0  1382.0   4.0  15.0  2.0  181.0  ...  e5ba7672   \n",
       "1      0  2.0    0  44.0   1.0   102.0   8.0   2.0  2.0    4.0  ...  07c540c4   \n",
       "2      0  2.0    0   1.0  14.0   767.0  89.0   4.0  2.0  245.0  ...  8efede7f   \n",
       "3      0  NaN  893   NaN   NaN  4392.0   NaN   0.0  0.0    0.0  ...  1e88c74f   \n",
       "4      0  3.0   -1   NaN   0.0     2.0   0.0   3.0  0.0    0.0  ...  1e88c74f   \n",
       "\n",
       "        C18       C19       C20       C21       C22       C23       C24  \\\n",
       "0  f54016b9  21ddcdc9  b1252a9d  07b5194c       NaN  3a171ecb  c5c50484   \n",
       "1  b04e4670  21ddcdc9  5840adea  60f6221e       NaN  3a171ecb  43f13e8b   \n",
       "2  3412118d       NaN       NaN  e587c466  ad3062eb  3a171ecb  3b183c5c   \n",
       "3  74ef3502       NaN       NaN  6b3a5ca6       NaN  3a171ecb  9117a34a   \n",
       "4  26b3c7a7       NaN       NaN  21c9516a       NaN  32c7478e  b34f3128   \n",
       "\n",
       "        C25       C26  \n",
       "0  e8b83407  9727dd16  \n",
       "1  e8b83407  731c3655  \n",
       "2       NaN       NaN  \n",
       "3       NaN       NaN  \n",
       "4       NaN       NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据类型概览:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Data columns (total 40 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   label   int64  \n",
      " 1   I1      float64\n",
      " 2   I2      int64  \n",
      " 3   I3      float64\n",
      " 4   I4      float64\n",
      " 5   I5      float64\n",
      " 6   I6      float64\n",
      " 7   I7      float64\n",
      " 8   I8      float64\n",
      " 9   I9      float64\n",
      " 10  I10     float64\n",
      " 11  I11     float64\n",
      " 12  I12     float64\n",
      " 13  I13     float64\n",
      " 14  C1      object \n",
      " 15  C2      object \n",
      " 16  C3      object \n",
      " 17  C4      object \n",
      " 18  C5      object \n",
      " 19  C6      object \n",
      " 20  C7      object \n",
      " 21  C8      object \n",
      " 22  C9      object \n",
      " 23  C10     object \n",
      " 24  C11     object \n",
      " 25  C12     object \n",
      " 26  C13     object \n",
      " 27  C14     object \n",
      " 28  C15     object \n",
      " 29  C16     object \n",
      " 30  C17     object \n",
      " 31  C18     object \n",
      " 32  C19     object \n",
      " 33  C20     object \n",
      " 34  C21     object \n",
      " 35  C22     object \n",
      " 36  C23     object \n",
      " 37  C24     object \n",
      " 38  C25     object \n",
      " 39  C26     object \n",
      "dtypes: float64(12), int64(2), object(26)\n",
      "memory usage: 610.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 定义列名 (Criteo数据集没有表头，我们需要自己加上)\n",
    "# Label: 0或1，表示是否被点击\n",
    "# I1-I13: 13个整数特征 (Integer Features)\n",
    "# C1-C26: 26个类别特征 (Categorical Features)\n",
    "cols = ['label'] + [f'I{i}' for i in range(1, 14)] + [f'C{i}' for i in range(1, 27)]\n",
    "\n",
    "# 2. 设置文件路径\n",
    "file_path = 'D:/PythonProjects/recommendation/kaggle-CTR/kaggle-display-advertising-challenge-dataset/train.txt' \n",
    "\n",
    "# 3. 读取前 10 万行看看长什么样\n",
    "# sep='\\t' 表示是用 Tab 键分隔的\n",
    "try:\n",
    "    df = pd.read_csv(file_path, sep='\\t', names=cols, nrows=2000000)\n",
    "    print(\"数据读取成功！形状为:\", df.shape)\n",
    "\n",
    "    # 展示前 5 行\n",
    "    display(df.head())\n",
    "\n",
    "    # 看看每一列的数据类型\n",
    "    print(\"\\n数据类型概览:\")\n",
    "    print(df.info())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"找不到文件，请检查 file_path 是否写对了。\")\n",
    "except Exception as e:\n",
    "    print(\"出错了:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bae3e1-7829-4a56-bfdb-abd0f3032ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label          0\n",
      "I1        849735\n",
      "I2             0\n",
      "I3        454348\n",
      "I4        473222\n",
      "I5         45417\n",
      "I6        437836\n",
      "I7         79899\n",
      "I8          1197\n",
      "I9         79899\n",
      "I10       849735\n",
      "I11        79899\n",
      "I12      1542352\n",
      "I13       473222\n",
      "C1             0\n",
      "C2             0\n",
      "C3         69482\n",
      "C4         69482\n",
      "C5             0\n",
      "C6        223355\n",
      "C7             0\n",
      "C8             0\n",
      "C9             0\n",
      "C10            0\n",
      "C11            0\n",
      "C12        69482\n",
      "C13            0\n",
      "C14            0\n",
      "C15            0\n",
      "C16        69482\n",
      "C17            0\n",
      "C18            0\n",
      "C19       941343\n",
      "C20       941343\n",
      "C21        69482\n",
      "C22      1465691\n",
      "C23            0\n",
      "C24        69482\n",
      "C25       941343\n",
      "C26       941343\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674dabe6-c41b-4b58-a1db-fd0f49e2eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数值特征标准化完成！\n",
      "剩余缺失值数量: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. 定义数值特征列名 \n",
    "dense_features = [f'I{i}' for i in range(1, 14)]\n",
    "sparse_features = [f'C{i}' for i in range(1, 27)]\n",
    "\n",
    "# 2. 处理数值特征 (I1-I13)\n",
    "# 第一步：空值填 0\n",
    "df[dense_features] = df[dense_features].fillna(0)\n",
    "\n",
    "########################################################################################################\n",
    "# ===  关键修改：用 StandardScaler 代替 Log 变换 ===\n",
    "# 这一步会把数值特征变成 0 附近的小数 (比如 -1.2, 0.5, 2.1)\n",
    "# 这样模型就不容易梯度爆炸了 \n",
    "scaler = StandardScaler()\n",
    "df[dense_features] = scaler.fit_transform(df[dense_features])\n",
    "\n",
    "# 3. 处理类别特征 (C1-C26): 空值填 \"-1\"\n",
    "df[sparse_features] = df[sparse_features].fillna('-1')\n",
    "\n",
    "# 4. 再次检查\n",
    "print(\"数值特征标准化完成！\")\n",
    "print(\"剩余缺失值数量:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab6af08-8b51-4b0a-a0d9-71f869849171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码完成，来看看现在的样子:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          label        I1        I2        I3        I4        I5        I6  \\\n",
       "0            0 -0.126108 -0.268473 -0.042678 -0.667604 -0.249399 -0.239015   \n",
       "1            0  0.016668 -0.271109  0.111554 -0.541726 -0.268222 -0.227721   \n",
       "2            0  0.016668 -0.271109 -0.058497  1.094679 -0.258443  0.000997   \n",
       "3            0 -0.268884  2.082906 -0.062451 -0.667604 -0.205136 -0.250310   \n",
       "4            0  0.159443 -0.273745 -0.062451 -0.667604 -0.269692 -0.250310   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1999995      1 -0.268884  1.002115 -0.062451 -0.415849 -0.037790 -0.128892   \n",
       "1999996      0  0.159443 -0.268473 -0.050587 -0.667604 -0.256604 -0.241839   \n",
       "1999997      1 -0.126108 -0.273745 -0.062451 -0.667604 -0.248590 -0.247486   \n",
       "1999998      1  1.444426 -0.271109  0.072007  0.842925 -0.268824 -0.213602   \n",
       "1999999      0 -0.268884 -0.271109 -0.054542 -0.415849 -0.227532 -0.148658   \n",
       "\n",
       "               I7        I8        I9  ...  C17   C18  C19  C20     C21  C22  \\\n",
       "0        0.023990 -0.442124  0.399715  ...    9  4053  237    3   13436    0   \n",
       "1       -0.206764 -0.442124 -0.453183  ...    0  2904  237    1  167950    0   \n",
       "2       -0.171263 -0.442124  0.708108  ...    6   850    0    0  397303   11   \n",
       "3       -0.242264 -0.521416 -0.472457  ...    1  1913    0    0  185717    0   \n",
       "4       -0.189013 -0.521416 -0.472457  ...    1   639    0    0   58474    0   \n",
       "...           ...       ...       ...  ...  ...   ...  ...  ...     ...  ...   \n",
       "1999995 -0.064761  0.073269 -0.202614  ...    4  1905    0    0  279527    0   \n",
       "1999996 -0.135762 -0.006023 -0.130334  ...    9  2014  333    3  379879    0   \n",
       "1999997 -0.100262 -0.481770 -0.183339  ...    9  3419  237    1  158611    0   \n",
       "1999998 -0.029261  0.033623 -0.414634  ...    9  2904  237    3  167950    0   \n",
       "1999999 -0.206764 -0.442124  1.589917  ...    0  3242    0    0     128    0   \n",
       "\n",
       "         C23    C24  C25    C26  \n",
       "0          3  46295   68  25504  \n",
       "1          3  15898   68  19236  \n",
       "2          3  13834    0      0  \n",
       "3          3  33810    0      0  \n",
       "4          2  41894    0      0  \n",
       "...      ...    ...  ...    ...  \n",
       "1999995    3  53615    0      0  \n",
       "1999996    2  56652    8  28488  \n",
       "1999997    3  23498   57  34752  \n",
       "1999998    2  15898   71  19236  \n",
       "1999999    2  13834    0      0  \n",
       "\n",
       "[2000000 rows x 40 columns]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 这里的字典用来保存每一列的 Encoder，以防以后预测时要用\n",
    "le_dict = {}\n",
    "\n",
    "for col in [f'C{i}' for i in range(1, 27)]:\n",
    "    le = LabelEncoder()\n",
    "    # fit_transform 会把这一列的字符串变成 0, 1, 2... 的数字\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "print(\"编码完成，来看看现在的样子:\")\n",
    "display(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab9a24a-5285-4cce-a803-b4353eb61d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在切分数据...\n",
      "训练集数量：1600000\n",
      "验证机数量：400000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. 定义列名\n",
    "dense_features = [f'I{i}' for i in range(1, 14)]\n",
    "sparse_features =[f'C{i}' for i in range(1, 27)]\n",
    "\n",
    "# 2. 提取数据 (转换为 numpy 数组)\n",
    "# label: 目标值\n",
    "# dense_x: 数值特征矩阵\n",
    "# sparse_x: 类别特征矩阵\n",
    "labels = df['label'].values\n",
    "dense_x = df[dense_features].values\n",
    "sparse_x = df[sparse_features].values\n",
    "\n",
    "# 3. 切分训练集和验证集 (8:2)\n",
    "print(\"正在切分数据...\")\n",
    "train_dense, val_dense, train_sparse, val_sparse, train_label, val_label =train_test_split(\n",
    "    dense_x, sparse_x, labels, test_size = 0.2, random_state = 2025\n",
    ")\n",
    "\n",
    "print(f\"训练集数量：{len(train_label)}\")\n",
    "print(f\"验证机数量：{len(val_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0565ca0b-09f6-466f-bea2-21019c1a4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriteoDataset(Dataset):\n",
    "    def __init__(self, dense_x, sparse_x, label):\n",
    "        \"\"\"\n",
    "        初始化函数：把 numpy 数组转成 tensor\n",
    "        \"\"\"\n",
    "        # 数值特征 -> float32\n",
    "        self.dense_x = torch.tensor(dense_x, dtype = torch.float32)\n",
    "        # 类别特征 -> long (必须是整数，否则 Embedding 层会报错)\n",
    "        self.sparse_x = torch.tensor(sparse_x, dtype = torch.long)\n",
    "        # 标签 -> float32 (为了配合 BCE Loss 计算)\n",
    "        self.label = torch.tensor(label, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 告诉 DataLoader 数据集有多长\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 告诉 DataLoader 怎么取第 idx 条数据\n",
    "        return self.dense_x[idx], self.sparse_x[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea23513a-07de-4c5c-8307-74db1ea05ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 管道测试报告 ===\n",
      "Dense Batch Shape: torch.Size([512, 13])  (预期[2048, 13])\n",
      "Sparse Batch Shape: torch.Size([512, 26])(预期[2048, 26])\n",
      "Label Batch Shape: torch.Size([512])  (预期[2048])\n",
      "√测试通过！数据管道搭建成功。\n"
     ]
    }
   ],
   "source": [
    "# 1. 实例化 Dataset\n",
    "train_dataset = CriteoDataset(train_dense, train_sparse, train_label)\n",
    "val_dataset = CriteoDataset(val_dense, val_sparse, val_label)\n",
    "\n",
    "# 2. 实例化 DataLoader\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0)\n",
    "\n",
    "# === ✅ 验证环节  ===\n",
    "# 从管道里取一个 Batch 出来看看形状\n",
    "dense_batch, sparse_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"\\n=== 管道测试报告 ===\")\n",
    "print(f\"Dense Batch Shape: {dense_batch.shape}  (预期[2048, 13])\")\n",
    "print(f\"Sparse Batch Shape: {sparse_batch.shape}(预期[2048, 26])\")\n",
    "print(f\"Label Batch Shape: {label_batch.shape}  (预期[2048])\")\n",
    "\n",
    "if dense_batch.shape == (BATCH_SIZE, 13) and sparse_batch.dtype == torch.int64:\n",
    "    print(\"√测试通过！数据管道搭建成功。\")\n",
    "else:\n",
    "    print(\"× 测试失败，请检查形状或数据类型。\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748ba3a2-9907-463c-92de-3e90ced028ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别特征词表大小示例：[1370, 541, 597026, 200785, 284] ...\n",
      "数值特征的数量：13\n"
     ]
    }
   ],
   "source": [
    "# 1. 获取类别特征的词表大小 (Vocabulary Size)\n",
    "# 也就是每一列有多少个不同的整数索引\n",
    "sparse_feat_sizes = [df[f'C{i}'].nunique() for i in range(1, 27)]\n",
    "\n",
    "# 2. 获取数值特征的数量\n",
    "dense_feat_num = len(dense_features)\n",
    "\n",
    "print(f\"类别特征词表大小示例：{sparse_feat_sizes[:5]} ...\")\n",
    "print(f\"数值特征的数量：{dense_feat_num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14160d6f-3012-4884-8e77-c5fa0bde6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFM 模型定义完成！\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, sparse_feat_sizes, dense_feat_num, embedding_dim = 8, hidden_units = [256, 128, 64], dropout = 0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_feat_sizes: 一个列表，包含26个类别特征各自的词表大小\n",
    "            dense_feat_num: 数值特征的数量 (13)\n",
    "            embedding_dim: Embedding 向量的长度 (k)，论文通常设为 8 或 10\n",
    "            hidden_units: DNN 的层数和每层神经元数量\n",
    "            dropout: 防止过拟合的概率\n",
    "        \"\"\"\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.dense_feat_num = dense_feat_num\n",
    "        self.sparse_feat_num = len(sparse_feat_sizes)\n",
    "        # === 1. Embedding 层 (Shared Input) ===\n",
    "            # 为每个类别特征建立一个 Embedding 表\n",
    "            # ModuleList 就像一个列表，里面存了 26 个 nn.Embedding\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for vocab_size in sparse_feat_sizes])\n",
    "        # === 2. FM 部分 (First Order & Second Order) ===\n",
    "            # 一阶 (Linear) 部分：\n",
    "            # 类别特征的一阶权重：本质上就是 Embedding_dim=1 的 Embedding\n",
    "        self.fm_1st_order_sparse = nn.ModuleList([nn.Embedding(vocab_size, 1) for vocab_size in sparse_feat_sizes\n",
    "                                             ])\n",
    "        # 数值特征的一阶权重：全连接层 13 -> 1\n",
    "        self.fm_1st_order_dense = nn.Linear(dense_feat_num, 1)\n",
    "\n",
    "        # 二阶 (Interaction) 部分：不需要额外的参数，直接用上面的 self.embeddings 计算\n",
    "\n",
    "        # === 3. DNN 部分 (Deep Component) ===\n",
    "            # 计算 DNN 的输入维度：\n",
    "            # 输入 = (类别特征数 * Embedding维度) + 数值特征数\n",
    "            # 比如：26 * 8 + 13 = 221\n",
    "        self.input_dim = self.sparse_feat_num * embedding_dim + dense_feat_num\n",
    "\n",
    "        layers = []\n",
    "        input_dim = self.input_dim\n",
    "        # 循环搭建全连接层 (Linear -> ReLU -> Dropout)\n",
    "        for unit in hidden_units:\n",
    "            layers.append(nn.Linear(input_dim, unit))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = unit\n",
    "        # 最后一层：映射到 1 个输出\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "\n",
    "        self.dnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, dense_x, sparse_x):\n",
    "        \"\"\"\n",
    "        dense_x: [batch_size, 13]\n",
    "        sparse_x: [batch_size, 26]\n",
    "        \"\"\"\n",
    "        # === A. Embedding 处理 ===\n",
    "        # 把 26 个 long 类型的索引变成 26 个向量\n",
    "        # embedded_sparse_features 形状: [batch, 26, embedding_dim]\n",
    "        embeds = [emb(sparse_x[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        '''之前没分清楚的一点是nn.Embedding和self.embeddings是不一样的，\n",
    "        self.embeddings是有着26个nn.Embedding的列表，\n",
    "        i取的就是self.embeddings对应的第i类的也就是Ci类对应的nn.Embedding。\n",
    "        而这个nn.Embedding有着N行样本，每个样本对应着隐向量。\n",
    "        这里相当于是i就是第i行或者self.embeddings的Ci类，\n",
    "        而emb实际上就是第i个nn.Embedding。\n",
    "        粗俗点理解就是i是self.embeddings的索引或者自变量，\n",
    "        而emb是因变量也就是对应索引对应的这个列表中的值。\n",
    "        而sparse_x本身存的就是这个batch里的id，\n",
    "        也就是nn.Embedding的索引或者“钥匙”。\n",
    "        这时候把一整行的sparse_x拉过来解锁，\n",
    "        第i列也就是第i类的全部在sparse_x里的id都用到了。\n",
    "        现在还没有具体的值，后面在训练过程中nn.Embedding里面的各类拥有的id会变。\n",
    "        emb不是什么列表元组，它就是对应的nn.Embedding，一个实例\n",
    "        而sparse_x就是数据集.那nn.embedding是什么类的，我知道他是一个矩阵等价于emb，不管那么多了，\n",
    "        反正就相当于提取sparse_x中的号码，可以在这个N*K的矩阵中取走号码对应的行\n",
    "        总结： DeepFM 通过 nn.Embedding 这一层，把“没法算”的 1 亿维稀疏 ID，变成了“好算”的 8 维稠密向量。\n",
    "        而且这个 8 维向量同时承载了 FM 的二阶交叉任务和 DNN 的高阶拟合任务，实现了**端到端（End-to-End）**的训练，\n",
    "        不再需要人工去组合特征 。\n",
    "        \n",
    "        '''\n",
    "        embeds = torch.stack(embeds, dim = 1) \n",
    "        '''embeds (张量) = [B, 26, K],在原本的第dim维度的地方插入一个新的维度，\n",
    "        沿着这个维度堆叠原来的26个独立张量。相当于新开一个轴，或者说类似于26张豆腐皮在一根烤串上\n",
    "        新的坐标轴（维度）可以索引这些豆腐皮。[样本, 特征, 向量] 的标准矩阵形式。'''\n",
    "\n",
    "        # === B. FM 一阶部分 (Linear) ===\n",
    "        # 1. 类别特征查表 (dim=1) -> 求和，这里用cat在原本的\"特征\"这一维度上concatenate连接而不同于上面的stack新开一个维度堆叠\n",
    "        fm_1st_sparse_res = [emb(sparse_x[:, i]) for i, emb in enumerate(self.fm_1st_order_sparse)]\n",
    "        fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim = 1)\n",
    "        fm_1st_sparse_sum = torch.sum(fm_1st_sparse_res, dim = 1, keepdim = True) # [batch, 1] True保持这个维度不被消灭，维持张量形状\n",
    "\n",
    "        # 2. 数值特征 Linear -> [batch, 1]\n",
    "        fm_1st_dense_res = self.fm_1st_order_dense(dense_x)\n",
    "\n",
    "        # 3. 合并\n",
    "        fm_1st_part = fm_1st_sparse_sum + fm_1st_dense_res\n",
    "\n",
    "        # === C. FM 二阶部分 (Interaction) ===\n",
    "        # 公式：0.5 * sum( (sum(vx))^2 - sum(v^2 * x^2) )\n",
    "        ###把两个隐向量的点积用数学手段变成这个公式，缩小计算量，其实就是高中的期望的公式的变体\n",
    "        # 这里 x=1 (因为我们只对类别特征做二阶交叉，且隐式 x=1) 数值特征没必要，简化了，直接用DNN就可以有交互\n",
    "\n",
    "        # 1. 和的平方\n",
    "        sum_square = torch.pow(torch.sum(embeds, dim = 1), 2)\n",
    "        # 2. 平方的和\n",
    "        square_sum = torch.sum(torch.pow(embeds, 2), dim = 1)\n",
    "        # 3. 相减除以2\n",
    "        fm_2nd_part = 0.5 * torch.sum(sum_square - square_sum, dim = 1, keepdim = True)\n",
    "\n",
    "        # === D. DNN 部分 ===\n",
    "        # 1. 把 Embedding 展平: [batch, 26 * 8]\n",
    "        #通过展平，我们成功地将 $26$ 个特征向量首尾相接，创建了一个包含所有嵌入信息的长向量（维度 $208$），这构成了 DNN 输入的稀疏特征部分。\n",
    "        batch_size = sparse_x.shape[0]\n",
    "        '''因为 sparse_x.shape 返回的是一个元组 (2048, 26)，\n",
    "        要获取元组中的第一个元素（即 $0$ 索引处的元素），\n",
    "        我们必须使用 中括号 [] 进行索引。[0] 代表获取元组中的第一个值，即 Batch Size。'''\n",
    "        flatten_embeds = embeds.view(batch_size, -1)\n",
    "        \n",
    "        \n",
    "        # 2. 拼接数值特征: [batch, 26*8 + 13] 沿着特征维度\n",
    "        dnn_input = torch.cat([flatten_embeds, dense_x], dim = 1)\n",
    "\n",
    "        # 3. 跑 MLP\n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "\n",
    "        # === E. 最终结果 (Sum) ===\n",
    "        # DeepFM = FM一阶 + FM二阶 + DNN输出\n",
    "        total_logit = fm_1st_part + fm_2nd_part + dnn_output\n",
    "\n",
    "        # 这里的输出是 Logit (还没有过 Sigmoid)，因为我们后面会用 BCEWithLogitsLoss\n",
    "        return total_logit\n",
    "\n",
    "print(\"DeepFM 模型定义完成！\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ce3e27-1d67-44f5-b056-75bf14772af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "模型参数已按 Xavier 和 Normal(0.01) 重新初始化！\n",
      "模型输出形状：torch.Size([512, 1]) (预期：[512, 1])\n",
      "√ 模型搭建成功！前向传播没问题。\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "###############################################################################\n",
    "# ===  1. 定义初始化权重函数 ===\n",
    "def init_weights(m):\n",
    "    # 如果是全连接层 (Linear) -> 用 Xavier 初始化\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    # 如果是 Embedding 层 -> 用方差极小的正态分布初始化 (防止梯度爆炸的关键!)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "\n",
    "# === 2. 准备设备 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === 3. 实例化模型 ===\n",
    "# 注意：hidden_units 里的参数名 hidden_units 要和你的 DeepFM 类定义一致(有没有s)\n",
    "model = DeepFM(sparse_feat_sizes, dense_feat_num, embedding_dim=8, hidden_units=[256, 128, 64]).to(device)\n",
    "\n",
    "# ===  4. 应用初始化  ===\n",
    "model.apply(init_weights)\n",
    "print(\"模型参数已按 Xavier 和 Normal(0.01) 重新初始化！\")\n",
    "\n",
    "# === 5. 测试 Forward ===\n",
    "# 注意：要把数据也搬到 GPU 上\n",
    "# 这里的 dense_batch 和 sparse_batch 来自你上一个 DataLoader Cell 的测试结果\n",
    "dense_batch = dense_batch.to(device)\n",
    "sparse_batch = sparse_batch.to(device)\n",
    "\n",
    "# 调用模型\n",
    "output = model(dense_batch, sparse_batch) \n",
    "\n",
    "# 自动获取当前的 Batch Size (防止你把 2048 改成 512 后这里报错)\n",
    "current_batch_size = dense_batch.shape[0]\n",
    "\n",
    "print(f\"模型输出形状：{output.shape} (预期：[{current_batch_size}, 1])\")\n",
    "\n",
    "if output.shape == (current_batch_size, 1):\n",
    "    print(\"√ 模型搭建成功！前向传播没问题。\")\n",
    "else:\n",
    "    print(\"× 模型输出形状不对，请检查代码。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6389f5-b4ba-46f6-89ae-2744a40c2633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Start Training for 15 epochs...\n",
      "\n",
      "--- Epoch 1 ---\n",
      "[Epoch 1] Step 50 / 3125 | Loss：12.1915\n",
      "[Epoch 1] Step 100 / 3125 | Loss：12.4327\n",
      "[Epoch 1] Step 150 / 3125 | Loss：9.0861\n",
      "[Epoch 1] Step 200 / 3125 | Loss：9.6159\n",
      "[Epoch 1] Step 250 / 3125 | Loss：8.7590\n",
      "[Epoch 1] Step 300 / 3125 | Loss：7.5285\n",
      "[Epoch 1] Step 350 / 3125 | Loss：9.0843\n",
      "[Epoch 1] Step 400 / 3125 | Loss：6.2896\n",
      "[Epoch 1] Step 450 / 3125 | Loss：6.4408\n",
      "[Epoch 1] Step 500 / 3125 | Loss：6.1225\n",
      "[Epoch 1] Step 550 / 3125 | Loss：6.0393\n",
      "[Epoch 1] Step 600 / 3125 | Loss：4.9132\n",
      "[Epoch 1] Step 650 / 3125 | Loss：5.5494\n",
      "[Epoch 1] Step 700 / 3125 | Loss：4.5136\n",
      "[Epoch 1] Step 750 / 3125 | Loss：4.9978\n",
      "[Epoch 1] Step 800 / 3125 | Loss：4.9524\n",
      "[Epoch 1] Step 850 / 3125 | Loss：4.3050\n",
      "[Epoch 1] Step 900 / 3125 | Loss：3.9119\n",
      "[Epoch 1] Step 950 / 3125 | Loss：4.2110\n",
      "[Epoch 1] Step 1000 / 3125 | Loss：3.9140\n",
      "[Epoch 1] Step 1050 / 3125 | Loss：4.3028\n",
      "[Epoch 1] Step 1100 / 3125 | Loss：3.5290\n",
      "[Epoch 1] Step 1150 / 3125 | Loss：3.6048\n",
      "[Epoch 1] Step 1200 / 3125 | Loss：4.6258\n",
      "[Epoch 1] Step 1250 / 3125 | Loss：4.1280\n",
      "[Epoch 1] Step 1300 / 3125 | Loss：3.3773\n",
      "[Epoch 1] Step 1350 / 3125 | Loss：3.2602\n",
      "[Epoch 1] Step 1400 / 3125 | Loss：3.4417\n",
      "[Epoch 1] Step 1450 / 3125 | Loss：3.5590\n",
      "[Epoch 1] Step 1500 / 3125 | Loss：3.4015\n",
      "[Epoch 1] Step 1550 / 3125 | Loss：2.8781\n",
      "[Epoch 1] Step 1600 / 3125 | Loss：3.3550\n",
      "[Epoch 1] Step 1650 / 3125 | Loss：2.9519\n",
      "[Epoch 1] Step 1700 / 3125 | Loss：2.9420\n",
      "[Epoch 1] Step 1750 / 3125 | Loss：2.5588\n",
      "[Epoch 1] Step 1800 / 3125 | Loss：3.1428\n",
      "[Epoch 1] Step 1850 / 3125 | Loss：2.4696\n",
      "[Epoch 1] Step 1900 / 3125 | Loss：2.7175\n",
      "[Epoch 1] Step 1950 / 3125 | Loss：2.8815\n",
      "[Epoch 1] Step 2000 / 3125 | Loss：3.3903\n",
      "[Epoch 1] Step 2050 / 3125 | Loss：2.5080\n",
      "[Epoch 1] Step 2100 / 3125 | Loss：2.5133\n",
      "[Epoch 1] Step 2150 / 3125 | Loss：2.4907\n",
      "[Epoch 1] Step 2200 / 3125 | Loss：2.9030\n",
      "[Epoch 1] Step 2250 / 3125 | Loss：2.0651\n",
      "[Epoch 1] Step 2300 / 3125 | Loss：2.5997\n",
      "[Epoch 1] Step 2350 / 3125 | Loss：2.4729\n",
      "[Epoch 1] Step 2400 / 3125 | Loss：2.3793\n",
      "[Epoch 1] Step 2450 / 3125 | Loss：2.3677\n",
      "[Epoch 1] Step 2500 / 3125 | Loss：2.5135\n",
      "[Epoch 1] Step 2550 / 3125 | Loss：2.3939\n",
      "[Epoch 1] Step 2600 / 3125 | Loss：2.5406\n",
      "[Epoch 1] Step 2650 / 3125 | Loss：2.2460\n",
      "[Epoch 1] Step 2700 / 3125 | Loss：2.2163\n",
      "[Epoch 1] Step 2750 / 3125 | Loss：2.2985\n",
      "[Epoch 1] Step 2800 / 3125 | Loss：3.4594\n",
      "[Epoch 1] Step 2850 / 3125 | Loss：2.0258\n",
      "[Epoch 1] Step 2900 / 3125 | Loss：1.9564\n",
      "[Epoch 1] Step 2950 / 3125 | Loss：2.1014\n",
      "[Epoch 1] Step 3000 / 3125 | Loss：2.6468\n",
      "[Epoch 1] Step 3050 / 3125 | Loss：2.2059\n",
      "[Epoch 1] Step 3100 / 3125 | Loss：1.8639\n",
      "Epoch 1 Result:\n",
      "  Train Loss: 4.2162\n",
      "  Val Loss:   1.6139\n",
      "  Val AUC:    0.6737\n",
      "\n",
      "--- Epoch 2 ---\n",
      "[Epoch 2] Step 50 / 3125 | Loss：2.0913\n",
      "[Epoch 2] Step 100 / 3125 | Loss：1.9051\n",
      "[Epoch 2] Step 150 / 3125 | Loss：2.0620\n",
      "[Epoch 2] Step 200 / 3125 | Loss：2.1222\n",
      "[Epoch 2] Step 250 / 3125 | Loss：1.8701\n",
      "[Epoch 2] Step 300 / 3125 | Loss：1.6307\n",
      "[Epoch 2] Step 350 / 3125 | Loss：1.6797\n",
      "[Epoch 2] Step 400 / 3125 | Loss：1.7450\n",
      "[Epoch 2] Step 450 / 3125 | Loss：2.0612\n",
      "[Epoch 2] Step 500 / 3125 | Loss：1.7960\n",
      "[Epoch 2] Step 550 / 3125 | Loss：1.7264\n",
      "[Epoch 2] Step 600 / 3125 | Loss：1.7224\n",
      "[Epoch 2] Step 650 / 3125 | Loss：1.7182\n",
      "[Epoch 2] Step 700 / 3125 | Loss：1.5933\n",
      "[Epoch 2] Step 750 / 3125 | Loss：1.5437\n",
      "[Epoch 2] Step 800 / 3125 | Loss：1.6122\n",
      "[Epoch 2] Step 850 / 3125 | Loss：1.8698\n",
      "[Epoch 2] Step 900 / 3125 | Loss：1.8768\n",
      "[Epoch 2] Step 950 / 3125 | Loss：1.9631\n",
      "[Epoch 2] Step 1000 / 3125 | Loss：1.4466\n",
      "[Epoch 2] Step 1050 / 3125 | Loss：1.6046\n",
      "[Epoch 2] Step 1100 / 3125 | Loss：1.5322\n",
      "[Epoch 2] Step 1150 / 3125 | Loss：1.4806\n",
      "[Epoch 2] Step 1200 / 3125 | Loss：1.3911\n",
      "[Epoch 2] Step 1250 / 3125 | Loss：1.5578\n",
      "[Epoch 2] Step 1300 / 3125 | Loss：1.5121\n",
      "[Epoch 2] Step 1350 / 3125 | Loss：1.4201\n",
      "[Epoch 2] Step 1400 / 3125 | Loss：1.7908\n",
      "[Epoch 2] Step 1450 / 3125 | Loss：1.4832\n",
      "[Epoch 2] Step 1500 / 3125 | Loss：1.7299\n",
      "[Epoch 2] Step 1550 / 3125 | Loss：1.4918\n",
      "[Epoch 2] Step 1600 / 3125 | Loss：1.3211\n",
      "[Epoch 2] Step 1650 / 3125 | Loss：1.4848\n",
      "[Epoch 2] Step 1700 / 3125 | Loss：1.4128\n",
      "[Epoch 2] Step 1750 / 3125 | Loss：1.4109\n",
      "[Epoch 2] Step 1800 / 3125 | Loss：1.4366\n",
      "[Epoch 2] Step 1850 / 3125 | Loss：1.3400\n",
      "[Epoch 2] Step 1900 / 3125 | Loss：1.5440\n",
      "[Epoch 2] Step 1950 / 3125 | Loss：1.3425\n",
      "[Epoch 2] Step 2000 / 3125 | Loss：1.2758\n",
      "[Epoch 2] Step 2050 / 3125 | Loss：1.4671\n",
      "[Epoch 2] Step 2100 / 3125 | Loss：1.1179\n",
      "[Epoch 2] Step 2150 / 3125 | Loss：1.2474\n",
      "[Epoch 2] Step 2200 / 3125 | Loss：1.3345\n",
      "[Epoch 2] Step 2250 / 3125 | Loss：1.2861\n",
      "[Epoch 2] Step 2300 / 3125 | Loss：1.0940\n",
      "[Epoch 2] Step 2350 / 3125 | Loss：1.3704\n",
      "[Epoch 2] Step 2400 / 3125 | Loss：1.7074\n",
      "[Epoch 2] Step 2450 / 3125 | Loss：1.3399\n",
      "[Epoch 2] Step 2500 / 3125 | Loss：1.1694\n",
      "[Epoch 2] Step 2550 / 3125 | Loss：1.3849\n",
      "[Epoch 2] Step 2600 / 3125 | Loss：0.9989\n",
      "[Epoch 2] Step 2650 / 3125 | Loss：1.1459\n",
      "[Epoch 2] Step 2700 / 3125 | Loss：1.3770\n",
      "[Epoch 2] Step 2750 / 3125 | Loss：1.3237\n",
      "[Epoch 2] Step 2800 / 3125 | Loss：1.1789\n",
      "[Epoch 2] Step 2850 / 3125 | Loss：1.2785\n",
      "[Epoch 2] Step 2900 / 3125 | Loss：1.1118\n",
      "[Epoch 2] Step 2950 / 3125 | Loss：1.2404\n",
      "[Epoch 2] Step 3000 / 3125 | Loss：1.1428\n",
      "[Epoch 2] Step 3050 / 3125 | Loss：1.1461\n",
      "[Epoch 2] Step 3100 / 3125 | Loss：0.9900\n",
      "Epoch 2 Result:\n",
      "  Train Loss: 1.5059\n",
      "  Val Loss:   0.9132\n",
      "  Val AUC:    0.7006\n",
      "\n",
      "--- Epoch 3 ---\n",
      "[Epoch 3] Step 50 / 3125 | Loss：0.9533\n",
      "[Epoch 3] Step 100 / 3125 | Loss：0.8603\n",
      "[Epoch 3] Step 150 / 3125 | Loss：0.9740\n",
      "[Epoch 3] Step 200 / 3125 | Loss：1.0757\n",
      "[Epoch 3] Step 250 / 3125 | Loss：0.9678\n",
      "[Epoch 3] Step 300 / 3125 | Loss：1.0298\n",
      "[Epoch 3] Step 350 / 3125 | Loss：0.9867\n",
      "[Epoch 3] Step 400 / 3125 | Loss：0.9921\n",
      "[Epoch 3] Step 450 / 3125 | Loss：0.8515\n",
      "[Epoch 3] Step 500 / 3125 | Loss：0.9582\n",
      "[Epoch 3] Step 550 / 3125 | Loss：1.0041\n",
      "[Epoch 3] Step 600 / 3125 | Loss：1.0137\n",
      "[Epoch 3] Step 650 / 3125 | Loss：0.9672\n",
      "[Epoch 3] Step 700 / 3125 | Loss：0.8184\n",
      "[Epoch 3] Step 750 / 3125 | Loss：1.0064\n",
      "[Epoch 3] Step 800 / 3125 | Loss：0.9989\n",
      "[Epoch 3] Step 850 / 3125 | Loss：0.9983\n",
      "[Epoch 3] Step 900 / 3125 | Loss：0.9519\n",
      "[Epoch 3] Step 950 / 3125 | Loss：1.0279\n",
      "[Epoch 3] Step 1000 / 3125 | Loss：1.0211\n",
      "[Epoch 3] Step 1050 / 3125 | Loss：0.9133\n",
      "[Epoch 3] Step 1100 / 3125 | Loss：0.9519\n",
      "[Epoch 3] Step 1150 / 3125 | Loss：0.9573\n",
      "[Epoch 3] Step 1200 / 3125 | Loss：0.8358\n",
      "[Epoch 3] Step 1250 / 3125 | Loss：0.7895\n",
      "[Epoch 3] Step 1300 / 3125 | Loss：0.8599\n",
      "[Epoch 3] Step 1350 / 3125 | Loss：0.9524\n",
      "[Epoch 3] Step 1400 / 3125 | Loss：0.7892\n",
      "[Epoch 3] Step 1450 / 3125 | Loss：0.7994\n",
      "[Epoch 3] Step 1500 / 3125 | Loss：0.9486\n",
      "[Epoch 3] Step 1550 / 3125 | Loss：0.9100\n",
      "[Epoch 3] Step 1600 / 3125 | Loss：0.8914\n",
      "[Epoch 3] Step 1650 / 3125 | Loss：0.8341\n",
      "[Epoch 3] Step 1700 / 3125 | Loss：0.8088\n",
      "[Epoch 3] Step 1750 / 3125 | Loss：0.8181\n",
      "[Epoch 3] Step 1800 / 3125 | Loss：0.9921\n",
      "[Epoch 3] Step 1850 / 3125 | Loss：0.7289\n",
      "[Epoch 3] Step 1900 / 3125 | Loss：0.9217\n",
      "[Epoch 3] Step 1950 / 3125 | Loss：0.8486\n",
      "[Epoch 3] Step 2000 / 3125 | Loss：0.8749\n",
      "[Epoch 3] Step 2050 / 3125 | Loss：0.7693\n",
      "[Epoch 3] Step 2100 / 3125 | Loss：0.8002\n",
      "[Epoch 3] Step 2150 / 3125 | Loss：0.8545\n",
      "[Epoch 3] Step 2200 / 3125 | Loss：0.7887\n",
      "[Epoch 3] Step 2250 / 3125 | Loss：0.8275\n",
      "[Epoch 3] Step 2300 / 3125 | Loss：0.8909\n",
      "[Epoch 3] Step 2350 / 3125 | Loss：0.7592\n",
      "[Epoch 3] Step 2400 / 3125 | Loss：0.8048\n",
      "[Epoch 3] Step 2450 / 3125 | Loss：0.8111\n",
      "[Epoch 3] Step 2500 / 3125 | Loss：0.6560\n",
      "[Epoch 3] Step 2550 / 3125 | Loss：0.6960\n",
      "[Epoch 3] Step 2600 / 3125 | Loss：0.7935\n",
      "[Epoch 3] Step 2650 / 3125 | Loss：0.8285\n",
      "[Epoch 3] Step 2700 / 3125 | Loss：0.7756\n",
      "[Epoch 3] Step 2750 / 3125 | Loss：0.8192\n",
      "[Epoch 3] Step 2800 / 3125 | Loss：0.7454\n",
      "[Epoch 3] Step 2850 / 3125 | Loss：0.8118\n",
      "[Epoch 3] Step 2900 / 3125 | Loss：0.7863\n",
      "[Epoch 3] Step 2950 / 3125 | Loss：0.8039\n",
      "[Epoch 3] Step 3000 / 3125 | Loss：0.7133\n",
      "[Epoch 3] Step 3050 / 3125 | Loss：0.8083\n",
      "[Epoch 3] Step 3100 / 3125 | Loss：0.6886\n",
      "Epoch 3 Result:\n",
      "  Train Loss: 0.8827\n",
      "  Val Loss:   0.6671\n",
      "  Val AUC:    0.7151\n",
      "\n",
      "--- Epoch 4 ---\n",
      "[Epoch 4] Step 50 / 3125 | Loss：0.6379\n",
      "[Epoch 4] Step 100 / 3125 | Loss：0.7104\n",
      "[Epoch 4] Step 150 / 3125 | Loss：0.6222\n",
      "[Epoch 4] Step 200 / 3125 | Loss：0.6470\n",
      "[Epoch 4] Step 250 / 3125 | Loss：0.5689\n",
      "[Epoch 4] Step 300 / 3125 | Loss：0.7203\n",
      "[Epoch 4] Step 350 / 3125 | Loss：0.6152\n",
      "[Epoch 4] Step 400 / 3125 | Loss：0.5989\n",
      "[Epoch 4] Step 450 / 3125 | Loss：0.6759\n",
      "[Epoch 4] Step 500 / 3125 | Loss：0.6461\n",
      "[Epoch 4] Step 550 / 3125 | Loss：0.6669\n",
      "[Epoch 4] Step 600 / 3125 | Loss：0.6352\n",
      "[Epoch 4] Step 650 / 3125 | Loss：0.6748\n",
      "[Epoch 4] Step 700 / 3125 | Loss：0.6185\n",
      "[Epoch 4] Step 750 / 3125 | Loss：0.5810\n",
      "[Epoch 4] Step 800 / 3125 | Loss：0.7113\n",
      "[Epoch 4] Step 850 / 3125 | Loss：0.6546\n",
      "[Epoch 4] Step 900 / 3125 | Loss：0.6648\n",
      "[Epoch 4] Step 950 / 3125 | Loss：0.6866\n",
      "[Epoch 4] Step 1000 / 3125 | Loss：0.6182\n",
      "[Epoch 4] Step 1050 / 3125 | Loss：0.5795\n",
      "[Epoch 4] Step 1100 / 3125 | Loss：0.7074\n",
      "[Epoch 4] Step 1150 / 3125 | Loss：0.6007\n",
      "[Epoch 4] Step 1200 / 3125 | Loss：0.6321\n",
      "[Epoch 4] Step 1250 / 3125 | Loss：0.5946\n",
      "[Epoch 4] Step 1300 / 3125 | Loss：0.5795\n",
      "[Epoch 4] Step 1350 / 3125 | Loss：0.5888\n",
      "[Epoch 4] Step 1400 / 3125 | Loss：0.6559\n",
      "[Epoch 4] Step 1450 / 3125 | Loss：0.6642\n",
      "[Epoch 4] Step 1500 / 3125 | Loss：0.6070\n",
      "[Epoch 4] Step 1550 / 3125 | Loss：0.6491\n",
      "[Epoch 4] Step 1600 / 3125 | Loss：0.6700\n",
      "[Epoch 4] Step 1650 / 3125 | Loss：0.6323\n",
      "[Epoch 4] Step 1700 / 3125 | Loss：0.6875\n",
      "[Epoch 4] Step 1750 / 3125 | Loss：0.6278\n",
      "[Epoch 4] Step 1800 / 3125 | Loss：0.6670\n",
      "[Epoch 4] Step 1850 / 3125 | Loss：0.5804\n",
      "[Epoch 4] Step 1900 / 3125 | Loss：0.6177\n",
      "[Epoch 4] Step 1950 / 3125 | Loss：0.5979\n",
      "[Epoch 4] Step 2000 / 3125 | Loss：0.5655\n",
      "[Epoch 4] Step 2050 / 3125 | Loss：0.6513\n",
      "[Epoch 4] Step 2100 / 3125 | Loss：0.6699\n",
      "[Epoch 4] Step 2150 / 3125 | Loss：0.5820\n",
      "[Epoch 4] Step 2200 / 3125 | Loss：0.5690\n",
      "[Epoch 4] Step 2250 / 3125 | Loss：0.6343\n",
      "[Epoch 4] Step 2300 / 3125 | Loss：0.5712\n",
      "[Epoch 4] Step 2350 / 3125 | Loss：0.6335\n",
      "[Epoch 4] Step 2400 / 3125 | Loss：0.5977\n",
      "[Epoch 4] Step 2450 / 3125 | Loss：0.6250\n",
      "[Epoch 4] Step 2500 / 3125 | Loss：0.6438\n",
      "[Epoch 4] Step 2550 / 3125 | Loss：0.6190\n",
      "[Epoch 4] Step 2600 / 3125 | Loss：0.5823\n",
      "[Epoch 4] Step 2650 / 3125 | Loss：0.6443\n",
      "[Epoch 4] Step 2700 / 3125 | Loss：0.5904\n",
      "[Epoch 4] Step 2750 / 3125 | Loss：0.5699\n",
      "[Epoch 4] Step 2800 / 3125 | Loss：0.5809\n",
      "[Epoch 4] Step 2850 / 3125 | Loss：0.5753\n",
      "[Epoch 4] Step 2900 / 3125 | Loss：0.6312\n",
      "[Epoch 4] Step 2950 / 3125 | Loss：0.5533\n",
      "[Epoch 4] Step 3000 / 3125 | Loss：0.5434\n",
      "[Epoch 4] Step 3050 / 3125 | Loss：0.5866\n",
      "[Epoch 4] Step 3100 / 3125 | Loss：0.5654\n",
      "Epoch 4 Result:\n",
      "  Train Loss: 0.6271\n",
      "  Val Loss:   0.5628\n",
      "  Val AUC:    0.7330\n",
      "\n",
      "--- Epoch 5 ---\n",
      "[Epoch 5] Step 50 / 3125 | Loss：0.4378\n",
      "[Epoch 5] Step 100 / 3125 | Loss：0.4624\n",
      "[Epoch 5] Step 150 / 3125 | Loss：0.5650\n",
      "[Epoch 5] Step 200 / 3125 | Loss：0.5320\n",
      "[Epoch 5] Step 250 / 3125 | Loss：0.5456\n",
      "[Epoch 5] Step 300 / 3125 | Loss：0.5224\n",
      "[Epoch 5] Step 350 / 3125 | Loss：0.5128\n",
      "[Epoch 5] Step 400 / 3125 | Loss：0.5610\n",
      "[Epoch 5] Step 450 / 3125 | Loss：0.4893\n",
      "[Epoch 5] Step 500 / 3125 | Loss：0.5107\n",
      "[Epoch 5] Step 550 / 3125 | Loss：0.5807\n",
      "[Epoch 5] Step 600 / 3125 | Loss：0.4606\n",
      "[Epoch 5] Step 650 / 3125 | Loss：0.5290\n",
      "[Epoch 5] Step 700 / 3125 | Loss：0.5142\n",
      "[Epoch 5] Step 750 / 3125 | Loss：0.4560\n",
      "[Epoch 5] Step 800 / 3125 | Loss：0.5972\n",
      "[Epoch 5] Step 850 / 3125 | Loss：0.5268\n",
      "[Epoch 5] Step 900 / 3125 | Loss：0.4907\n",
      "[Epoch 5] Step 950 / 3125 | Loss：0.5291\n",
      "[Epoch 5] Step 1000 / 3125 | Loss：0.5310\n",
      "[Epoch 5] Step 1050 / 3125 | Loss：0.5372\n",
      "[Epoch 5] Step 1100 / 3125 | Loss：0.5109\n",
      "[Epoch 5] Step 1150 / 3125 | Loss：0.4602\n",
      "[Epoch 5] Step 1200 / 3125 | Loss：0.4318\n",
      "[Epoch 5] Step 1250 / 3125 | Loss：0.5944\n",
      "[Epoch 5] Step 1300 / 3125 | Loss：0.5039\n",
      "[Epoch 5] Step 1350 / 3125 | Loss：0.5014\n",
      "[Epoch 5] Step 1400 / 3125 | Loss：0.5265\n",
      "[Epoch 5] Step 1450 / 3125 | Loss：0.5257\n",
      "[Epoch 5] Step 1500 / 3125 | Loss：0.5046\n",
      "[Epoch 5] Step 1550 / 3125 | Loss：0.5419\n",
      "[Epoch 5] Step 1600 / 3125 | Loss：0.5583\n",
      "[Epoch 5] Step 1650 / 3125 | Loss：0.5131\n",
      "[Epoch 5] Step 1700 / 3125 | Loss：0.5547\n",
      "[Epoch 5] Step 1750 / 3125 | Loss：0.4534\n",
      "[Epoch 5] Step 1800 / 3125 | Loss：0.4406\n",
      "[Epoch 5] Step 1850 / 3125 | Loss：0.5476\n",
      "[Epoch 5] Step 1900 / 3125 | Loss：0.4810\n",
      "[Epoch 5] Step 1950 / 3125 | Loss：0.5490\n",
      "[Epoch 5] Step 2000 / 3125 | Loss：0.5277\n",
      "[Epoch 5] Step 2050 / 3125 | Loss：0.5303\n",
      "[Epoch 5] Step 2100 / 3125 | Loss：0.5313\n",
      "[Epoch 5] Step 2150 / 3125 | Loss：0.4745\n",
      "[Epoch 5] Step 2200 / 3125 | Loss：0.4882\n",
      "[Epoch 5] Step 2250 / 3125 | Loss：0.5327\n",
      "[Epoch 5] Step 2300 / 3125 | Loss：0.4980\n",
      "[Epoch 5] Step 2350 / 3125 | Loss：0.5350\n",
      "[Epoch 5] Step 2400 / 3125 | Loss：0.5028\n",
      "[Epoch 5] Step 2450 / 3125 | Loss：0.4665\n",
      "[Epoch 5] Step 2500 / 3125 | Loss：0.4914\n",
      "[Epoch 5] Step 2550 / 3125 | Loss：0.5659\n",
      "[Epoch 5] Step 2600 / 3125 | Loss：0.5216\n",
      "[Epoch 5] Step 2650 / 3125 | Loss：0.5066\n",
      "[Epoch 5] Step 2700 / 3125 | Loss：0.5013\n",
      "[Epoch 5] Step 2750 / 3125 | Loss：0.5264\n",
      "[Epoch 5] Step 2800 / 3125 | Loss：0.5195\n",
      "[Epoch 5] Step 2850 / 3125 | Loss：0.5429\n",
      "[Epoch 5] Step 2900 / 3125 | Loss：0.5919\n",
      "[Epoch 5] Step 2950 / 3125 | Loss：0.5458\n",
      "[Epoch 5] Step 3000 / 3125 | Loss：0.5018\n",
      "[Epoch 5] Step 3050 / 3125 | Loss：0.5060\n",
      "[Epoch 5] Step 3100 / 3125 | Loss：0.5348\n",
      "Epoch 5 Result:\n",
      "  Train Loss: 0.5198\n",
      "  Val Loss:   0.5145\n",
      "  Val AUC:    0.7474\n",
      "\n",
      "--- Epoch 6 ---\n",
      "[Epoch 6] Step 50 / 3125 | Loss：0.4377\n",
      "[Epoch 6] Step 100 / 3125 | Loss：0.4484\n",
      "[Epoch 6] Step 150 / 3125 | Loss：0.4751\n",
      "[Epoch 6] Step 200 / 3125 | Loss：0.4269\n",
      "[Epoch 6] Step 250 / 3125 | Loss：0.4248\n",
      "[Epoch 6] Step 300 / 3125 | Loss：0.4401\n",
      "[Epoch 6] Step 350 / 3125 | Loss：0.4543\n",
      "[Epoch 6] Step 400 / 3125 | Loss：0.4415\n",
      "[Epoch 6] Step 450 / 3125 | Loss：0.4621\n",
      "[Epoch 6] Step 500 / 3125 | Loss：0.4507\n",
      "[Epoch 6] Step 550 / 3125 | Loss：0.4989\n",
      "[Epoch 6] Step 600 / 3125 | Loss：0.4772\n",
      "[Epoch 6] Step 650 / 3125 | Loss：0.4828\n",
      "[Epoch 6] Step 700 / 3125 | Loss：0.4600\n",
      "[Epoch 6] Step 750 / 3125 | Loss：0.4897\n",
      "[Epoch 6] Step 800 / 3125 | Loss：0.4158\n",
      "[Epoch 6] Step 850 / 3125 | Loss：0.4610\n",
      "[Epoch 6] Step 900 / 3125 | Loss：0.5068\n",
      "[Epoch 6] Step 950 / 3125 | Loss：0.5138\n",
      "[Epoch 6] Step 1000 / 3125 | Loss：0.5320\n",
      "[Epoch 6] Step 1050 / 3125 | Loss：0.5643\n",
      "[Epoch 6] Step 1100 / 3125 | Loss：0.5308\n",
      "[Epoch 6] Step 1150 / 3125 | Loss：0.4598\n",
      "[Epoch 6] Step 1200 / 3125 | Loss：0.4923\n",
      "[Epoch 6] Step 1250 / 3125 | Loss：0.5265\n",
      "[Epoch 6] Step 1300 / 3125 | Loss：0.4549\n",
      "[Epoch 6] Step 1350 / 3125 | Loss：0.4947\n",
      "[Epoch 6] Step 1400 / 3125 | Loss：0.4090\n",
      "[Epoch 6] Step 1450 / 3125 | Loss：0.5036\n",
      "[Epoch 6] Step 1500 / 3125 | Loss：0.4196\n",
      "[Epoch 6] Step 1550 / 3125 | Loss：0.5157\n",
      "[Epoch 6] Step 1600 / 3125 | Loss：0.4605\n",
      "[Epoch 6] Step 1650 / 3125 | Loss：0.4853\n",
      "[Epoch 6] Step 1700 / 3125 | Loss：0.5338\n",
      "[Epoch 6] Step 1750 / 3125 | Loss：0.5013\n",
      "[Epoch 6] Step 1800 / 3125 | Loss：0.5292\n",
      "[Epoch 6] Step 1850 / 3125 | Loss：0.4770\n",
      "[Epoch 6] Step 1900 / 3125 | Loss：0.5147\n",
      "[Epoch 6] Step 1950 / 3125 | Loss：0.5476\n",
      "[Epoch 6] Step 2000 / 3125 | Loss：0.4688\n",
      "[Epoch 6] Step 2050 / 3125 | Loss：0.4794\n",
      "[Epoch 6] Step 2100 / 3125 | Loss：0.4882\n",
      "[Epoch 6] Step 2150 / 3125 | Loss：0.4548\n",
      "[Epoch 6] Step 2200 / 3125 | Loss：0.4690\n",
      "[Epoch 6] Step 2250 / 3125 | Loss：0.4553\n",
      "[Epoch 6] Step 2300 / 3125 | Loss：0.5182\n",
      "[Epoch 6] Step 2350 / 3125 | Loss：0.5003\n",
      "[Epoch 6] Step 2400 / 3125 | Loss：0.5103\n",
      "[Epoch 6] Step 2450 / 3125 | Loss：0.4173\n",
      "[Epoch 6] Step 2500 / 3125 | Loss：0.4467\n",
      "[Epoch 6] Step 2550 / 3125 | Loss：0.4390\n",
      "[Epoch 6] Step 2600 / 3125 | Loss：0.4581\n",
      "[Epoch 6] Step 2650 / 3125 | Loss：0.4853\n",
      "[Epoch 6] Step 2700 / 3125 | Loss：0.4422\n",
      "[Epoch 6] Step 2750 / 3125 | Loss：0.4790\n",
      "[Epoch 6] Step 2800 / 3125 | Loss：0.4644\n",
      "[Epoch 6] Step 2850 / 3125 | Loss：0.4644\n",
      "[Epoch 6] Step 2900 / 3125 | Loss：0.5210\n",
      "[Epoch 6] Step 2950 / 3125 | Loss：0.4396\n",
      "[Epoch 6] Step 3000 / 3125 | Loss：0.4930\n",
      "[Epoch 6] Step 3050 / 3125 | Loss：0.4577\n",
      "[Epoch 6] Step 3100 / 3125 | Loss：0.4836\n",
      "Epoch 6 Result:\n",
      "  Train Loss: 0.4746\n",
      "  Val Loss:   0.4918\n",
      "  Val AUC:    0.7579\n",
      "\n",
      "--- Epoch 7 ---\n",
      "[Epoch 7] Step 50 / 3125 | Loss：0.4147\n",
      "[Epoch 7] Step 100 / 3125 | Loss：0.4065\n",
      "[Epoch 7] Step 150 / 3125 | Loss：0.4136\n",
      "[Epoch 7] Step 200 / 3125 | Loss：0.4340\n",
      "[Epoch 7] Step 250 / 3125 | Loss：0.4459\n",
      "[Epoch 7] Step 300 / 3125 | Loss：0.4776\n",
      "[Epoch 7] Step 350 / 3125 | Loss：0.4655\n",
      "[Epoch 7] Step 400 / 3125 | Loss：0.4704\n",
      "[Epoch 7] Step 450 / 3125 | Loss：0.3926\n",
      "[Epoch 7] Step 500 / 3125 | Loss：0.4366\n",
      "[Epoch 7] Step 550 / 3125 | Loss：0.4407\n",
      "[Epoch 7] Step 600 / 3125 | Loss：0.4145\n",
      "[Epoch 7] Step 650 / 3125 | Loss：0.4588\n",
      "[Epoch 7] Step 700 / 3125 | Loss：0.4366\n",
      "[Epoch 7] Step 750 / 3125 | Loss：0.4362\n",
      "[Epoch 7] Step 800 / 3125 | Loss：0.4087\n",
      "[Epoch 7] Step 850 / 3125 | Loss：0.4872\n",
      "[Epoch 7] Step 900 / 3125 | Loss：0.4197\n",
      "[Epoch 7] Step 950 / 3125 | Loss：0.4528\n",
      "[Epoch 7] Step 1000 / 3125 | Loss：0.5008\n",
      "[Epoch 7] Step 1050 / 3125 | Loss：0.4984\n",
      "[Epoch 7] Step 1100 / 3125 | Loss：0.4589\n",
      "[Epoch 7] Step 1150 / 3125 | Loss：0.3858\n",
      "[Epoch 7] Step 1200 / 3125 | Loss：0.4466\n",
      "[Epoch 7] Step 1250 / 3125 | Loss：0.4393\n",
      "[Epoch 7] Step 1300 / 3125 | Loss：0.4483\n",
      "[Epoch 7] Step 1350 / 3125 | Loss：0.4142\n",
      "[Epoch 7] Step 1400 / 3125 | Loss：0.4617\n",
      "[Epoch 7] Step 1450 / 3125 | Loss：0.4835\n",
      "[Epoch 7] Step 1500 / 3125 | Loss：0.4730\n",
      "[Epoch 7] Step 1550 / 3125 | Loss：0.4641\n",
      "[Epoch 7] Step 1600 / 3125 | Loss：0.4816\n",
      "[Epoch 7] Step 1650 / 3125 | Loss：0.5114\n",
      "[Epoch 7] Step 1700 / 3125 | Loss：0.4756\n",
      "[Epoch 7] Step 1750 / 3125 | Loss：0.4705\n",
      "[Epoch 7] Step 1800 / 3125 | Loss：0.4819\n",
      "[Epoch 7] Step 1850 / 3125 | Loss：0.4874\n",
      "[Epoch 7] Step 1900 / 3125 | Loss：0.4430\n",
      "[Epoch 7] Step 1950 / 3125 | Loss：0.5155\n",
      "[Epoch 7] Step 2000 / 3125 | Loss：0.4846\n",
      "[Epoch 7] Step 2050 / 3125 | Loss：0.4770\n",
      "[Epoch 7] Step 2100 / 3125 | Loss：0.4279\n",
      "[Epoch 7] Step 2150 / 3125 | Loss：0.4979\n",
      "[Epoch 7] Step 2200 / 3125 | Loss：0.5032\n",
      "[Epoch 7] Step 2250 / 3125 | Loss：0.5001\n",
      "[Epoch 7] Step 2300 / 3125 | Loss：0.4600\n",
      "[Epoch 7] Step 2350 / 3125 | Loss：0.4488\n",
      "[Epoch 7] Step 2400 / 3125 | Loss：0.4738\n",
      "[Epoch 7] Step 2450 / 3125 | Loss：0.4553\n",
      "[Epoch 7] Step 2500 / 3125 | Loss：0.4480\n",
      "[Epoch 7] Step 2550 / 3125 | Loss：0.4596\n",
      "[Epoch 7] Step 2600 / 3125 | Loss：0.4559\n",
      "[Epoch 7] Step 2650 / 3125 | Loss：0.4557\n",
      "[Epoch 7] Step 2700 / 3125 | Loss：0.4873\n",
      "[Epoch 7] Step 2750 / 3125 | Loss：0.4606\n",
      "[Epoch 7] Step 2800 / 3125 | Loss：0.4499\n",
      "[Epoch 7] Step 2850 / 3125 | Loss：0.4994\n",
      "[Epoch 7] Step 2900 / 3125 | Loss：0.4735\n",
      "[Epoch 7] Step 2950 / 3125 | Loss：0.5299\n",
      "[Epoch 7] Step 3000 / 3125 | Loss：0.4598\n",
      "[Epoch 7] Step 3050 / 3125 | Loss：0.4594\n",
      "[Epoch 7] Step 3100 / 3125 | Loss：0.4952\n",
      "Epoch 7 Result:\n",
      "  Train Loss: 0.4548\n",
      "  Val Loss:   0.4791\n",
      "  Val AUC:    0.7681\n",
      "\n",
      "--- Epoch 8 ---\n",
      "[Epoch 8] Step 50 / 3125 | Loss：0.4345\n",
      "[Epoch 8] Step 100 / 3125 | Loss：0.4417\n",
      "[Epoch 8] Step 150 / 3125 | Loss：0.4450\n",
      "[Epoch 8] Step 200 / 3125 | Loss：0.4024\n",
      "[Epoch 8] Step 250 / 3125 | Loss：0.4497\n",
      "[Epoch 8] Step 300 / 3125 | Loss：0.4672\n",
      "[Epoch 8] Step 350 / 3125 | Loss：0.4400\n",
      "[Epoch 8] Step 400 / 3125 | Loss：0.3992\n",
      "[Epoch 8] Step 450 / 3125 | Loss：0.4650\n",
      "[Epoch 8] Step 500 / 3125 | Loss：0.3965\n",
      "[Epoch 8] Step 550 / 3125 | Loss：0.4372\n",
      "[Epoch 8] Step 600 / 3125 | Loss：0.4480\n",
      "[Epoch 8] Step 650 / 3125 | Loss：0.4143\n",
      "[Epoch 8] Step 700 / 3125 | Loss：0.4495\n",
      "[Epoch 8] Step 750 / 3125 | Loss：0.4560\n",
      "[Epoch 8] Step 800 / 3125 | Loss：0.3822\n",
      "[Epoch 8] Step 850 / 3125 | Loss：0.4783\n",
      "[Epoch 8] Step 900 / 3125 | Loss：0.4125\n",
      "[Epoch 8] Step 950 / 3125 | Loss：0.4430\n",
      "[Epoch 8] Step 1000 / 3125 | Loss：0.4384\n",
      "[Epoch 8] Step 1050 / 3125 | Loss：0.4274\n",
      "[Epoch 8] Step 1100 / 3125 | Loss：0.4497\n",
      "[Epoch 8] Step 1150 / 3125 | Loss：0.4282\n",
      "[Epoch 8] Step 1200 / 3125 | Loss：0.4443\n",
      "[Epoch 8] Step 1250 / 3125 | Loss：0.4682\n",
      "[Epoch 8] Step 1300 / 3125 | Loss：0.4547\n",
      "[Epoch 8] Step 1350 / 3125 | Loss：0.4457\n",
      "[Epoch 8] Step 1400 / 3125 | Loss：0.4437\n",
      "[Epoch 8] Step 1450 / 3125 | Loss：0.4033\n",
      "[Epoch 8] Step 1500 / 3125 | Loss：0.4288\n",
      "[Epoch 8] Step 1550 / 3125 | Loss：0.4237\n",
      "[Epoch 8] Step 1600 / 3125 | Loss：0.4803\n",
      "[Epoch 8] Step 1650 / 3125 | Loss：0.4900\n",
      "[Epoch 8] Step 1700 / 3125 | Loss：0.4258\n",
      "[Epoch 8] Step 1750 / 3125 | Loss：0.4059\n",
      "[Epoch 8] Step 1800 / 3125 | Loss：0.4102\n",
      "[Epoch 8] Step 1850 / 3125 | Loss：0.4629\n",
      "[Epoch 8] Step 1900 / 3125 | Loss：0.3619\n",
      "[Epoch 8] Step 1950 / 3125 | Loss：0.4667\n",
      "[Epoch 8] Step 2000 / 3125 | Loss：0.4464\n",
      "[Epoch 8] Step 2050 / 3125 | Loss：0.4757\n",
      "[Epoch 8] Step 2100 / 3125 | Loss：0.4489\n",
      "[Epoch 8] Step 2150 / 3125 | Loss：0.4561\n",
      "[Epoch 8] Step 2200 / 3125 | Loss：0.4742\n",
      "[Epoch 8] Step 2250 / 3125 | Loss：0.4725\n",
      "[Epoch 8] Step 2300 / 3125 | Loss：0.4718\n",
      "[Epoch 8] Step 2350 / 3125 | Loss：0.4732\n",
      "[Epoch 8] Step 2400 / 3125 | Loss：0.4313\n",
      "[Epoch 8] Step 2450 / 3125 | Loss：0.4589\n",
      "[Epoch 8] Step 2500 / 3125 | Loss：0.4616\n",
      "[Epoch 8] Step 2550 / 3125 | Loss：0.4832\n",
      "[Epoch 8] Step 2600 / 3125 | Loss：0.4414\n",
      "[Epoch 8] Step 2650 / 3125 | Loss：0.5078\n",
      "[Epoch 8] Step 2700 / 3125 | Loss：0.4593\n",
      "[Epoch 8] Step 2750 / 3125 | Loss：0.4794\n",
      "[Epoch 8] Step 2800 / 3125 | Loss：0.4443\n",
      "[Epoch 8] Step 2850 / 3125 | Loss：0.4701\n",
      "[Epoch 8] Step 2900 / 3125 | Loss：0.4543\n",
      "[Epoch 8] Step 2950 / 3125 | Loss：0.4487\n",
      "[Epoch 8] Step 3000 / 3125 | Loss：0.4518\n",
      "[Epoch 8] Step 3050 / 3125 | Loss：0.4927\n",
      "[Epoch 8] Step 3100 / 3125 | Loss：0.4749\n",
      "Epoch 8 Result:\n",
      "  Train Loss: 0.4450\n",
      "  Val Loss:   0.4741\n",
      "  Val AUC:    0.7722\n",
      "\n",
      "--- Epoch 9 ---\n",
      "[Epoch 9] Step 50 / 3125 | Loss：0.4065\n",
      "[Epoch 9] Step 100 / 3125 | Loss：0.3913\n",
      "[Epoch 9] Step 150 / 3125 | Loss：0.3723\n",
      "[Epoch 9] Step 200 / 3125 | Loss：0.4365\n",
      "[Epoch 9] Step 250 / 3125 | Loss：0.3979\n",
      "[Epoch 9] Step 300 / 3125 | Loss：0.4113\n",
      "[Epoch 9] Step 350 / 3125 | Loss：0.4319\n",
      "[Epoch 9] Step 400 / 3125 | Loss：0.3703\n",
      "[Epoch 9] Step 450 / 3125 | Loss：0.4061\n",
      "[Epoch 9] Step 500 / 3125 | Loss：0.4190\n",
      "[Epoch 9] Step 550 / 3125 | Loss：0.4411\n",
      "[Epoch 9] Step 600 / 3125 | Loss：0.3883\n",
      "[Epoch 9] Step 650 / 3125 | Loss：0.4154\n",
      "[Epoch 9] Step 700 / 3125 | Loss：0.4069\n",
      "[Epoch 9] Step 750 / 3125 | Loss：0.4643\n",
      "[Epoch 9] Step 800 / 3125 | Loss：0.4260\n",
      "[Epoch 9] Step 850 / 3125 | Loss：0.3988\n",
      "[Epoch 9] Step 900 / 3125 | Loss：0.3941\n",
      "[Epoch 9] Step 950 / 3125 | Loss：0.4304\n",
      "[Epoch 9] Step 1000 / 3125 | Loss：0.4108\n",
      "[Epoch 9] Step 1050 / 3125 | Loss：0.4897\n",
      "[Epoch 9] Step 1100 / 3125 | Loss：0.4145\n",
      "[Epoch 9] Step 1150 / 3125 | Loss：0.4173\n",
      "[Epoch 9] Step 1200 / 3125 | Loss：0.4618\n",
      "[Epoch 9] Step 1250 / 3125 | Loss：0.4432\n",
      "[Epoch 9] Step 1300 / 3125 | Loss：0.4275\n",
      "[Epoch 9] Step 1350 / 3125 | Loss：0.4464\n",
      "[Epoch 9] Step 1400 / 3125 | Loss：0.4216\n",
      "[Epoch 9] Step 1450 / 3125 | Loss：0.4441\n",
      "[Epoch 9] Step 1500 / 3125 | Loss：0.4647\n",
      "[Epoch 9] Step 1550 / 3125 | Loss：0.4636\n",
      "[Epoch 9] Step 1600 / 3125 | Loss：0.4575\n",
      "[Epoch 9] Step 1650 / 3125 | Loss：0.4399\n",
      "[Epoch 9] Step 1700 / 3125 | Loss：0.4349\n",
      "[Epoch 9] Step 1750 / 3125 | Loss：0.4163\n",
      "[Epoch 9] Step 1800 / 3125 | Loss：0.4677\n",
      "[Epoch 9] Step 1850 / 3125 | Loss：0.4052\n",
      "[Epoch 9] Step 1900 / 3125 | Loss：0.4633\n",
      "[Epoch 9] Step 1950 / 3125 | Loss：0.4858\n",
      "[Epoch 9] Step 2000 / 3125 | Loss：0.4546\n",
      "[Epoch 9] Step 2050 / 3125 | Loss：0.4360\n",
      "[Epoch 9] Step 2100 / 3125 | Loss：0.4789\n",
      "[Epoch 9] Step 2150 / 3125 | Loss：0.4627\n",
      "[Epoch 9] Step 2200 / 3125 | Loss：0.4674\n",
      "[Epoch 9] Step 2250 / 3125 | Loss：0.4271\n",
      "[Epoch 9] Step 2300 / 3125 | Loss：0.4574\n",
      "[Epoch 9] Step 2350 / 3125 | Loss：0.4503\n",
      "[Epoch 9] Step 2400 / 3125 | Loss：0.4078\n",
      "[Epoch 9] Step 2450 / 3125 | Loss：0.4502\n",
      "[Epoch 9] Step 2500 / 3125 | Loss：0.4478\n",
      "[Epoch 9] Step 2550 / 3125 | Loss：0.4918\n",
      "[Epoch 9] Step 2600 / 3125 | Loss：0.4582\n",
      "[Epoch 9] Step 2650 / 3125 | Loss：0.4155\n",
      "[Epoch 9] Step 2700 / 3125 | Loss：0.4595\n",
      "[Epoch 9] Step 2750 / 3125 | Loss：0.4700\n",
      "[Epoch 9] Step 2800 / 3125 | Loss：0.4687\n",
      "[Epoch 9] Step 2850 / 3125 | Loss：0.4793\n",
      "[Epoch 9] Step 2900 / 3125 | Loss：0.4244\n",
      "[Epoch 9] Step 2950 / 3125 | Loss：0.4213\n",
      "[Epoch 9] Step 3000 / 3125 | Loss：0.4589\n",
      "[Epoch 9] Step 3050 / 3125 | Loss：0.4331\n",
      "[Epoch 9] Step 3100 / 3125 | Loss：0.4587\n",
      "Epoch 9 Result:\n",
      "  Train Loss: 0.4392\n",
      "  Val Loss:   0.4708\n",
      "  Val AUC:    0.7755\n",
      "\n",
      "--- Epoch 10 ---\n",
      "[Epoch 10] Step 50 / 3125 | Loss：0.4335\n",
      "[Epoch 10] Step 100 / 3125 | Loss：0.4120\n",
      "[Epoch 10] Step 150 / 3125 | Loss：0.3985\n",
      "[Epoch 10] Step 200 / 3125 | Loss：0.3996\n",
      "[Epoch 10] Step 250 / 3125 | Loss：0.4005\n",
      "[Epoch 10] Step 300 / 3125 | Loss：0.4256\n",
      "[Epoch 10] Step 350 / 3125 | Loss：0.3827\n",
      "[Epoch 10] Step 400 / 3125 | Loss：0.4226\n",
      "[Epoch 10] Step 450 / 3125 | Loss：0.4147\n",
      "[Epoch 10] Step 500 / 3125 | Loss：0.4271\n",
      "[Epoch 10] Step 550 / 3125 | Loss：0.4342\n",
      "[Epoch 10] Step 600 / 3125 | Loss：0.4218\n",
      "[Epoch 10] Step 650 / 3125 | Loss：0.4173\n",
      "[Epoch 10] Step 700 / 3125 | Loss：0.4205\n",
      "[Epoch 10] Step 750 / 3125 | Loss：0.4495\n",
      "[Epoch 10] Step 800 / 3125 | Loss：0.4580\n",
      "[Epoch 10] Step 850 / 3125 | Loss：0.4525\n",
      "[Epoch 10] Step 900 / 3125 | Loss：0.4375\n",
      "[Epoch 10] Step 950 / 3125 | Loss：0.4346\n",
      "[Epoch 10] Step 1000 / 3125 | Loss：0.4414\n",
      "[Epoch 10] Step 1050 / 3125 | Loss：0.4245\n",
      "[Epoch 10] Step 1100 / 3125 | Loss：0.4419\n",
      "[Epoch 10] Step 1150 / 3125 | Loss：0.4484\n",
      "[Epoch 10] Step 1200 / 3125 | Loss：0.4367\n",
      "[Epoch 10] Step 1250 / 3125 | Loss：0.4250\n",
      "[Epoch 10] Step 1300 / 3125 | Loss：0.4461\n",
      "[Epoch 10] Step 1350 / 3125 | Loss：0.4354\n",
      "[Epoch 10] Step 1400 / 3125 | Loss：0.3959\n",
      "[Epoch 10] Step 1450 / 3125 | Loss：0.5025\n",
      "[Epoch 10] Step 1500 / 3125 | Loss：0.4531\n",
      "[Epoch 10] Step 1550 / 3125 | Loss：0.4296\n",
      "[Epoch 10] Step 1600 / 3125 | Loss：0.4115\n",
      "[Epoch 10] Step 1650 / 3125 | Loss：0.4850\n",
      "[Epoch 10] Step 1700 / 3125 | Loss：0.4462\n",
      "[Epoch 10] Step 1750 / 3125 | Loss：0.4528\n",
      "[Epoch 10] Step 1800 / 3125 | Loss：0.4437\n",
      "[Epoch 10] Step 1850 / 3125 | Loss：0.4308\n",
      "[Epoch 10] Step 1900 / 3125 | Loss：0.4085\n",
      "[Epoch 10] Step 1950 / 3125 | Loss：0.4272\n",
      "[Epoch 10] Step 2000 / 3125 | Loss：0.4424\n",
      "[Epoch 10] Step 2050 / 3125 | Loss：0.4204\n",
      "[Epoch 10] Step 2100 / 3125 | Loss：0.4354\n",
      "[Epoch 10] Step 2150 / 3125 | Loss：0.4987\n",
      "[Epoch 10] Step 2200 / 3125 | Loss：0.4472\n",
      "[Epoch 10] Step 2250 / 3125 | Loss：0.4339\n",
      "[Epoch 10] Step 2300 / 3125 | Loss：0.4624\n",
      "[Epoch 10] Step 2350 / 3125 | Loss：0.4577\n",
      "[Epoch 10] Step 2400 / 3125 | Loss：0.4594\n",
      "[Epoch 10] Step 2450 / 3125 | Loss：0.4999\n",
      "[Epoch 10] Step 2500 / 3125 | Loss：0.4164\n",
      "[Epoch 10] Step 2550 / 3125 | Loss：0.4138\n",
      "[Epoch 10] Step 2600 / 3125 | Loss：0.4201\n",
      "[Epoch 10] Step 2650 / 3125 | Loss：0.4444\n",
      "[Epoch 10] Step 2700 / 3125 | Loss：0.4961\n",
      "[Epoch 10] Step 2750 / 3125 | Loss：0.4226\n",
      "[Epoch 10] Step 2800 / 3125 | Loss：0.4741\n",
      "[Epoch 10] Step 2850 / 3125 | Loss：0.4539\n",
      "[Epoch 10] Step 2900 / 3125 | Loss：0.4383\n",
      "[Epoch 10] Step 2950 / 3125 | Loss：0.4659\n",
      "[Epoch 10] Step 3000 / 3125 | Loss：0.4561\n",
      "[Epoch 10] Step 3050 / 3125 | Loss：0.5101\n",
      "[Epoch 10] Step 3100 / 3125 | Loss：0.4089\n",
      "Epoch 10 Result:\n",
      "  Train Loss: 0.4350\n",
      "  Val Loss:   0.4694\n",
      "  Val AUC:    0.7773\n",
      "\n",
      "--- Epoch 11 ---\n",
      "[Epoch 11] Step 50 / 3125 | Loss：0.4138\n",
      "[Epoch 11] Step 100 / 3125 | Loss：0.4036\n",
      "[Epoch 11] Step 150 / 3125 | Loss：0.3628\n",
      "[Epoch 11] Step 200 / 3125 | Loss：0.3784\n",
      "[Epoch 11] Step 250 / 3125 | Loss：0.4094\n",
      "[Epoch 11] Step 300 / 3125 | Loss：0.4366\n",
      "[Epoch 11] Step 350 / 3125 | Loss：0.4092\n",
      "[Epoch 11] Step 400 / 3125 | Loss：0.3970\n",
      "[Epoch 11] Step 450 / 3125 | Loss：0.4521\n",
      "[Epoch 11] Step 500 / 3125 | Loss：0.3853\n",
      "[Epoch 11] Step 550 / 3125 | Loss：0.4107\n",
      "[Epoch 11] Step 600 / 3125 | Loss：0.3746\n",
      "[Epoch 11] Step 650 / 3125 | Loss：0.4193\n",
      "[Epoch 11] Step 700 / 3125 | Loss：0.4013\n",
      "[Epoch 11] Step 750 / 3125 | Loss：0.4215\n",
      "[Epoch 11] Step 800 / 3125 | Loss：0.4383\n",
      "[Epoch 11] Step 850 / 3125 | Loss：0.4134\n",
      "[Epoch 11] Step 900 / 3125 | Loss：0.4476\n",
      "[Epoch 11] Step 950 / 3125 | Loss：0.4104\n",
      "[Epoch 11] Step 1000 / 3125 | Loss：0.4110\n",
      "[Epoch 11] Step 1050 / 3125 | Loss：0.4144\n",
      "[Epoch 11] Step 1100 / 3125 | Loss：0.4099\n",
      "[Epoch 11] Step 1150 / 3125 | Loss：0.4337\n",
      "[Epoch 11] Step 1200 / 3125 | Loss：0.4456\n",
      "[Epoch 11] Step 1250 / 3125 | Loss：0.3938\n",
      "[Epoch 11] Step 1300 / 3125 | Loss：0.4371\n",
      "[Epoch 11] Step 1350 / 3125 | Loss：0.4229\n",
      "[Epoch 11] Step 1400 / 3125 | Loss：0.4189\n",
      "[Epoch 11] Step 1450 / 3125 | Loss：0.4143\n",
      "[Epoch 11] Step 1500 / 3125 | Loss：0.4268\n",
      "[Epoch 11] Step 1550 / 3125 | Loss：0.4711\n",
      "[Epoch 11] Step 1600 / 3125 | Loss：0.4102\n",
      "[Epoch 11] Step 1650 / 3125 | Loss：0.4705\n",
      "[Epoch 11] Step 1700 / 3125 | Loss：0.3926\n",
      "[Epoch 11] Step 1750 / 3125 | Loss：0.4394\n",
      "[Epoch 11] Step 1800 / 3125 | Loss：0.4234\n",
      "[Epoch 11] Step 1850 / 3125 | Loss：0.4061\n",
      "[Epoch 11] Step 1900 / 3125 | Loss：0.4980\n",
      "[Epoch 11] Step 1950 / 3125 | Loss：0.4103\n",
      "[Epoch 11] Step 2000 / 3125 | Loss：0.4203\n",
      "[Epoch 11] Step 2050 / 3125 | Loss：0.4667\n",
      "[Epoch 11] Step 2100 / 3125 | Loss：0.4174\n",
      "[Epoch 11] Step 2150 / 3125 | Loss：0.4351\n",
      "[Epoch 11] Step 2200 / 3125 | Loss：0.4309\n",
      "[Epoch 11] Step 2250 / 3125 | Loss：0.4710\n",
      "[Epoch 11] Step 2300 / 3125 | Loss：0.4300\n",
      "[Epoch 11] Step 2350 / 3125 | Loss：0.4231\n",
      "[Epoch 11] Step 2400 / 3125 | Loss：0.4489\n",
      "[Epoch 11] Step 2450 / 3125 | Loss：0.4876\n",
      "[Epoch 11] Step 2500 / 3125 | Loss：0.4241\n",
      "[Epoch 11] Step 2550 / 3125 | Loss：0.4563\n",
      "[Epoch 11] Step 2600 / 3125 | Loss：0.4279\n",
      "[Epoch 11] Step 2650 / 3125 | Loss：0.4281\n",
      "[Epoch 11] Step 2700 / 3125 | Loss：0.4314\n",
      "[Epoch 11] Step 2750 / 3125 | Loss：0.4641\n",
      "[Epoch 11] Step 2800 / 3125 | Loss：0.4223\n",
      "[Epoch 11] Step 2850 / 3125 | Loss：0.4651\n",
      "[Epoch 11] Step 2900 / 3125 | Loss：0.4709\n",
      "[Epoch 11] Step 2950 / 3125 | Loss：0.4276\n",
      "[Epoch 11] Step 3000 / 3125 | Loss：0.4294\n",
      "[Epoch 11] Step 3050 / 3125 | Loss：0.4474\n",
      "[Epoch 11] Step 3100 / 3125 | Loss：0.3943\n",
      "Epoch 11 Result:\n",
      "  Train Loss: 0.4318\n",
      "  Val Loss:   0.4674\n",
      "  Val AUC:    0.7785\n",
      "\n",
      "--- Epoch 12 ---\n",
      "[Epoch 12] Step 50 / 3125 | Loss：0.3560\n",
      "[Epoch 12] Step 100 / 3125 | Loss：0.4259\n",
      "[Epoch 12] Step 150 / 3125 | Loss：0.3953\n",
      "[Epoch 12] Step 200 / 3125 | Loss：0.4284\n",
      "[Epoch 12] Step 250 / 3125 | Loss：0.4183\n",
      "[Epoch 12] Step 300 / 3125 | Loss：0.3786\n",
      "[Epoch 12] Step 350 / 3125 | Loss：0.3915\n",
      "[Epoch 12] Step 400 / 3125 | Loss：0.4437\n",
      "[Epoch 12] Step 450 / 3125 | Loss：0.4254\n",
      "[Epoch 12] Step 500 / 3125 | Loss：0.4033\n",
      "[Epoch 12] Step 550 / 3125 | Loss：0.4131\n",
      "[Epoch 12] Step 600 / 3125 | Loss：0.4320\n",
      "[Epoch 12] Step 650 / 3125 | Loss：0.4039\n",
      "[Epoch 12] Step 700 / 3125 | Loss：0.4253\n",
      "[Epoch 12] Step 750 / 3125 | Loss：0.4381\n",
      "[Epoch 12] Step 800 / 3125 | Loss：0.4054\n",
      "[Epoch 12] Step 850 / 3125 | Loss：0.3947\n",
      "[Epoch 12] Step 900 / 3125 | Loss：0.4168\n",
      "[Epoch 12] Step 950 / 3125 | Loss：0.4050\n",
      "[Epoch 12] Step 1000 / 3125 | Loss：0.4087\n",
      "[Epoch 12] Step 1050 / 3125 | Loss：0.3996\n",
      "[Epoch 12] Step 1100 / 3125 | Loss：0.4235\n",
      "[Epoch 12] Step 1150 / 3125 | Loss：0.4308\n",
      "[Epoch 12] Step 1200 / 3125 | Loss：0.3918\n",
      "[Epoch 12] Step 1250 / 3125 | Loss：0.3935\n",
      "[Epoch 12] Step 1300 / 3125 | Loss：0.4241\n",
      "[Epoch 12] Step 1350 / 3125 | Loss：0.4096\n",
      "[Epoch 12] Step 1400 / 3125 | Loss：0.4095\n",
      "[Epoch 12] Step 1450 / 3125 | Loss：0.4255\n",
      "[Epoch 12] Step 1500 / 3125 | Loss：0.4234\n",
      "[Epoch 12] Step 1550 / 3125 | Loss：0.3777\n",
      "[Epoch 12] Step 1600 / 3125 | Loss：0.4181\n",
      "[Epoch 12] Step 1650 / 3125 | Loss：0.4119\n",
      "[Epoch 12] Step 1700 / 3125 | Loss：0.4750\n",
      "[Epoch 12] Step 1750 / 3125 | Loss：0.4376\n",
      "[Epoch 12] Step 1800 / 3125 | Loss：0.4419\n",
      "[Epoch 12] Step 1850 / 3125 | Loss：0.4424\n",
      "[Epoch 12] Step 1900 / 3125 | Loss：0.4824\n",
      "[Epoch 12] Step 1950 / 3125 | Loss：0.4611\n",
      "[Epoch 12] Step 2000 / 3125 | Loss：0.4641\n",
      "[Epoch 12] Step 2050 / 3125 | Loss：0.4130\n",
      "[Epoch 12] Step 2100 / 3125 | Loss：0.4825\n",
      "[Epoch 12] Step 2150 / 3125 | Loss：0.3926\n",
      "[Epoch 12] Step 2200 / 3125 | Loss：0.4393\n",
      "[Epoch 12] Step 2250 / 3125 | Loss：0.4217\n",
      "[Epoch 12] Step 2300 / 3125 | Loss：0.4560\n",
      "[Epoch 12] Step 2350 / 3125 | Loss：0.4520\n",
      "[Epoch 12] Step 2400 / 3125 | Loss：0.4888\n",
      "[Epoch 12] Step 2450 / 3125 | Loss：0.4026\n",
      "[Epoch 12] Step 2500 / 3125 | Loss：0.4583\n",
      "[Epoch 12] Step 2550 / 3125 | Loss：0.4127\n",
      "[Epoch 12] Step 2600 / 3125 | Loss：0.4140\n",
      "[Epoch 12] Step 2650 / 3125 | Loss：0.4635\n",
      "[Epoch 12] Step 2700 / 3125 | Loss：0.4659\n",
      "[Epoch 12] Step 2750 / 3125 | Loss：0.4616\n",
      "[Epoch 12] Step 2800 / 3125 | Loss：0.4402\n",
      "[Epoch 12] Step 2850 / 3125 | Loss：0.4583\n",
      "[Epoch 12] Step 2900 / 3125 | Loss：0.4142\n",
      "[Epoch 12] Step 2950 / 3125 | Loss：0.4731\n",
      "[Epoch 12] Step 3000 / 3125 | Loss：0.3989\n",
      "[Epoch 12] Step 3050 / 3125 | Loss：0.4251\n",
      "[Epoch 12] Step 3100 / 3125 | Loss：0.4430\n",
      "Epoch 12 Result:\n",
      "  Train Loss: 0.4290\n",
      "  Val Loss:   0.4668\n",
      "  Val AUC:    0.7792\n",
      "\n",
      "--- Epoch 13 ---\n",
      "[Epoch 13] Step 50 / 3125 | Loss：0.4011\n",
      "[Epoch 13] Step 100 / 3125 | Loss：0.4145\n",
      "[Epoch 13] Step 150 / 3125 | Loss：0.4110\n",
      "[Epoch 13] Step 200 / 3125 | Loss：0.4480\n",
      "[Epoch 13] Step 250 / 3125 | Loss：0.4185\n",
      "[Epoch 13] Step 300 / 3125 | Loss：0.3812\n",
      "[Epoch 13] Step 350 / 3125 | Loss：0.3824\n",
      "[Epoch 13] Step 400 / 3125 | Loss：0.4128\n",
      "[Epoch 13] Step 450 / 3125 | Loss：0.3712\n",
      "[Epoch 13] Step 500 / 3125 | Loss：0.4128\n",
      "[Epoch 13] Step 550 / 3125 | Loss：0.4129\n",
      "[Epoch 13] Step 600 / 3125 | Loss：0.4347\n",
      "[Epoch 13] Step 650 / 3125 | Loss：0.3684\n",
      "[Epoch 13] Step 700 / 3125 | Loss：0.4241\n",
      "[Epoch 13] Step 750 / 3125 | Loss：0.4058\n",
      "[Epoch 13] Step 800 / 3125 | Loss：0.4120\n",
      "[Epoch 13] Step 850 / 3125 | Loss：0.4129\n",
      "[Epoch 13] Step 900 / 3125 | Loss：0.4207\n",
      "[Epoch 13] Step 950 / 3125 | Loss：0.4160\n",
      "[Epoch 13] Step 1000 / 3125 | Loss：0.4080\n",
      "[Epoch 13] Step 1050 / 3125 | Loss：0.4090\n",
      "[Epoch 13] Step 1100 / 3125 | Loss：0.4295\n",
      "[Epoch 13] Step 1150 / 3125 | Loss：0.4462\n",
      "[Epoch 13] Step 1200 / 3125 | Loss：0.4210\n",
      "[Epoch 13] Step 1250 / 3125 | Loss：0.4264\n",
      "[Epoch 13] Step 1300 / 3125 | Loss：0.4169\n",
      "[Epoch 13] Step 1350 / 3125 | Loss：0.4143\n",
      "[Epoch 13] Step 1400 / 3125 | Loss：0.4516\n",
      "[Epoch 13] Step 1450 / 3125 | Loss：0.4202\n",
      "[Epoch 13] Step 1500 / 3125 | Loss：0.4293\n",
      "[Epoch 13] Step 1550 / 3125 | Loss：0.4794\n",
      "[Epoch 13] Step 1600 / 3125 | Loss：0.4736\n",
      "[Epoch 13] Step 1650 / 3125 | Loss：0.4138\n",
      "[Epoch 13] Step 1700 / 3125 | Loss：0.4717\n",
      "[Epoch 13] Step 1750 / 3125 | Loss：0.3892\n",
      "[Epoch 13] Step 1800 / 3125 | Loss：0.4626\n",
      "[Epoch 13] Step 1850 / 3125 | Loss：0.4308\n",
      "[Epoch 13] Step 1900 / 3125 | Loss：0.3731\n",
      "[Epoch 13] Step 1950 / 3125 | Loss：0.4716\n",
      "[Epoch 13] Step 2000 / 3125 | Loss：0.4535\n",
      "[Epoch 13] Step 2050 / 3125 | Loss：0.4169\n",
      "[Epoch 13] Step 2100 / 3125 | Loss：0.4409\n",
      "[Epoch 13] Step 2150 / 3125 | Loss：0.4510\n",
      "[Epoch 13] Step 2200 / 3125 | Loss：0.4334\n",
      "[Epoch 13] Step 2250 / 3125 | Loss：0.4533\n",
      "[Epoch 13] Step 2300 / 3125 | Loss：0.4484\n",
      "[Epoch 13] Step 2350 / 3125 | Loss：0.4200\n",
      "[Epoch 13] Step 2400 / 3125 | Loss：0.4493\n",
      "[Epoch 13] Step 2450 / 3125 | Loss：0.3749\n",
      "[Epoch 13] Step 2500 / 3125 | Loss：0.4427\n",
      "[Epoch 13] Step 2550 / 3125 | Loss：0.4264\n",
      "[Epoch 13] Step 2600 / 3125 | Loss：0.4219\n",
      "[Epoch 13] Step 2650 / 3125 | Loss：0.4175\n",
      "[Epoch 13] Step 2700 / 3125 | Loss：0.4652\n",
      "[Epoch 13] Step 2750 / 3125 | Loss：0.4413\n",
      "[Epoch 13] Step 2800 / 3125 | Loss：0.4546\n",
      "[Epoch 13] Step 2850 / 3125 | Loss：0.4558\n",
      "[Epoch 13] Step 2900 / 3125 | Loss：0.4477\n",
      "[Epoch 13] Step 2950 / 3125 | Loss：0.4531\n",
      "[Epoch 13] Step 3000 / 3125 | Loss：0.4382\n",
      "[Epoch 13] Step 3050 / 3125 | Loss：0.4532\n",
      "[Epoch 13] Step 3100 / 3125 | Loss：0.3846\n",
      "Epoch 13 Result:\n",
      "  Train Loss: 0.4262\n",
      "  Val Loss:   0.4657\n",
      "  Val AUC:    0.7814\n",
      "\n",
      "--- Epoch 14 ---\n",
      "[Epoch 14] Step 50 / 3125 | Loss：0.4304\n",
      "[Epoch 14] Step 100 / 3125 | Loss：0.3631\n",
      "[Epoch 14] Step 150 / 3125 | Loss：0.4760\n",
      "[Epoch 14] Step 200 / 3125 | Loss：0.4170\n",
      "[Epoch 14] Step 250 / 3125 | Loss：0.3958\n",
      "[Epoch 14] Step 300 / 3125 | Loss：0.4367\n",
      "[Epoch 14] Step 350 / 3125 | Loss：0.3789\n",
      "[Epoch 14] Step 400 / 3125 | Loss：0.4437\n",
      "[Epoch 14] Step 450 / 3125 | Loss：0.4155\n",
      "[Epoch 14] Step 500 / 3125 | Loss：0.4162\n",
      "[Epoch 14] Step 550 / 3125 | Loss：0.3915\n",
      "[Epoch 14] Step 600 / 3125 | Loss：0.3726\n",
      "[Epoch 14] Step 650 / 3125 | Loss：0.3944\n",
      "[Epoch 14] Step 700 / 3125 | Loss：0.3967\n",
      "[Epoch 14] Step 750 / 3125 | Loss：0.3958\n",
      "[Epoch 14] Step 800 / 3125 | Loss：0.3900\n",
      "[Epoch 14] Step 850 / 3125 | Loss：0.3918\n",
      "[Epoch 14] Step 900 / 3125 | Loss：0.3978\n",
      "[Epoch 14] Step 950 / 3125 | Loss：0.3803\n",
      "[Epoch 14] Step 1000 / 3125 | Loss：0.4252\n",
      "[Epoch 14] Step 1050 / 3125 | Loss：0.4368\n",
      "[Epoch 14] Step 1100 / 3125 | Loss：0.3998\n",
      "[Epoch 14] Step 1150 / 3125 | Loss：0.4283\n",
      "[Epoch 14] Step 1200 / 3125 | Loss：0.4395\n",
      "[Epoch 14] Step 1250 / 3125 | Loss：0.4404\n",
      "[Epoch 14] Step 1300 / 3125 | Loss：0.4386\n",
      "[Epoch 14] Step 1350 / 3125 | Loss：0.4927\n",
      "[Epoch 14] Step 1400 / 3125 | Loss：0.4238\n",
      "[Epoch 14] Step 1450 / 3125 | Loss：0.3701\n",
      "[Epoch 14] Step 1500 / 3125 | Loss：0.4497\n",
      "[Epoch 14] Step 1550 / 3125 | Loss：0.4171\n",
      "[Epoch 14] Step 1600 / 3125 | Loss：0.4393\n",
      "[Epoch 14] Step 1650 / 3125 | Loss：0.4381\n",
      "[Epoch 14] Step 1700 / 3125 | Loss：0.4656\n",
      "[Epoch 14] Step 1750 / 3125 | Loss：0.4454\n",
      "[Epoch 14] Step 1800 / 3125 | Loss：0.4327\n",
      "[Epoch 14] Step 1850 / 3125 | Loss：0.4207\n",
      "[Epoch 14] Step 1900 / 3125 | Loss：0.4161\n",
      "[Epoch 14] Step 1950 / 3125 | Loss：0.4438\n",
      "[Epoch 14] Step 2000 / 3125 | Loss：0.4359\n",
      "[Epoch 14] Step 2050 / 3125 | Loss：0.4572\n",
      "[Epoch 14] Step 2100 / 3125 | Loss：0.4631\n",
      "[Epoch 14] Step 2150 / 3125 | Loss：0.4053\n",
      "[Epoch 14] Step 2200 / 3125 | Loss：0.4678\n",
      "[Epoch 14] Step 2250 / 3125 | Loss：0.4486\n",
      "[Epoch 14] Step 2300 / 3125 | Loss：0.4398\n",
      "[Epoch 14] Step 2350 / 3125 | Loss：0.4215\n",
      "[Epoch 14] Step 2400 / 3125 | Loss：0.4322\n",
      "[Epoch 14] Step 2450 / 3125 | Loss：0.4584\n",
      "[Epoch 14] Step 2500 / 3125 | Loss：0.4635\n",
      "[Epoch 14] Step 2550 / 3125 | Loss：0.4510\n",
      "[Epoch 14] Step 2600 / 3125 | Loss：0.4314\n",
      "[Epoch 14] Step 2650 / 3125 | Loss：0.4390\n",
      "[Epoch 14] Step 2700 / 3125 | Loss：0.4448\n",
      "[Epoch 14] Step 2750 / 3125 | Loss：0.4381\n",
      "[Epoch 14] Step 2800 / 3125 | Loss：0.4437\n",
      "[Epoch 14] Step 2850 / 3125 | Loss：0.4235\n",
      "[Epoch 14] Step 2900 / 3125 | Loss：0.4532\n",
      "[Epoch 14] Step 2950 / 3125 | Loss：0.4713\n",
      "[Epoch 14] Step 3000 / 3125 | Loss：0.4169\n",
      "[Epoch 14] Step 3050 / 3125 | Loss：0.4850\n",
      "[Epoch 14] Step 3100 / 3125 | Loss：0.4854\n",
      "Epoch 14 Result:\n",
      "  Train Loss: 0.4240\n",
      "  Val Loss:   0.4655\n",
      "  Val AUC:    0.7826\n",
      "\n",
      "--- Epoch 15 ---\n",
      "[Epoch 15] Step 50 / 3125 | Loss：0.3973\n",
      "[Epoch 15] Step 100 / 3125 | Loss：0.3862\n",
      "[Epoch 15] Step 150 / 3125 | Loss：0.3837\n",
      "[Epoch 15] Step 200 / 3125 | Loss：0.3951\n",
      "[Epoch 15] Step 250 / 3125 | Loss：0.3648\n",
      "[Epoch 15] Step 300 / 3125 | Loss：0.4171\n",
      "[Epoch 15] Step 350 / 3125 | Loss：0.4132\n",
      "[Epoch 15] Step 400 / 3125 | Loss：0.4385\n",
      "[Epoch 15] Step 450 / 3125 | Loss：0.3549\n",
      "[Epoch 15] Step 500 / 3125 | Loss：0.4044\n",
      "[Epoch 15] Step 550 / 3125 | Loss：0.3905\n",
      "[Epoch 15] Step 600 / 3125 | Loss：0.4364\n",
      "[Epoch 15] Step 650 / 3125 | Loss：0.4536\n",
      "[Epoch 15] Step 700 / 3125 | Loss：0.3678\n",
      "[Epoch 15] Step 750 / 3125 | Loss：0.4135\n",
      "[Epoch 15] Step 800 / 3125 | Loss：0.3853\n",
      "[Epoch 15] Step 850 / 3125 | Loss：0.3867\n",
      "[Epoch 15] Step 900 / 3125 | Loss：0.3962\n",
      "[Epoch 15] Step 950 / 3125 | Loss：0.4243\n",
      "[Epoch 15] Step 1000 / 3125 | Loss：0.4346\n",
      "[Epoch 15] Step 1050 / 3125 | Loss：0.4119\n",
      "[Epoch 15] Step 1100 / 3125 | Loss：0.3930\n",
      "[Epoch 15] Step 1150 / 3125 | Loss：0.4030\n",
      "[Epoch 15] Step 1200 / 3125 | Loss：0.4263\n",
      "[Epoch 15] Step 1250 / 3125 | Loss：0.4213\n",
      "[Epoch 15] Step 1300 / 3125 | Loss：0.4243\n",
      "[Epoch 15] Step 1350 / 3125 | Loss：0.4140\n",
      "[Epoch 15] Step 1400 / 3125 | Loss：0.4183\n",
      "[Epoch 15] Step 1450 / 3125 | Loss：0.4529\n",
      "[Epoch 15] Step 1500 / 3125 | Loss：0.3857\n",
      "[Epoch 15] Step 1550 / 3125 | Loss：0.4656\n",
      "[Epoch 15] Step 1600 / 3125 | Loss：0.3850\n",
      "[Epoch 15] Step 1650 / 3125 | Loss：0.4805\n",
      "[Epoch 15] Step 1700 / 3125 | Loss：0.4106\n",
      "[Epoch 15] Step 1750 / 3125 | Loss：0.4051\n",
      "[Epoch 15] Step 1800 / 3125 | Loss：0.4248\n",
      "[Epoch 15] Step 1850 / 3125 | Loss：0.3934\n",
      "[Epoch 15] Step 1900 / 3125 | Loss：0.4190\n",
      "[Epoch 15] Step 1950 / 3125 | Loss：0.4345\n",
      "[Epoch 15] Step 2000 / 3125 | Loss：0.4264\n",
      "[Epoch 15] Step 2050 / 3125 | Loss：0.4173\n",
      "[Epoch 15] Step 2100 / 3125 | Loss：0.4265\n",
      "[Epoch 15] Step 2150 / 3125 | Loss：0.4357\n",
      "[Epoch 15] Step 2200 / 3125 | Loss：0.4435\n",
      "[Epoch 15] Step 2250 / 3125 | Loss：0.4399\n",
      "[Epoch 15] Step 2300 / 3125 | Loss：0.4133\n",
      "[Epoch 15] Step 2350 / 3125 | Loss：0.4441\n",
      "[Epoch 15] Step 2400 / 3125 | Loss：0.4389\n",
      "[Epoch 15] Step 2450 / 3125 | Loss：0.4507\n",
      "[Epoch 15] Step 2500 / 3125 | Loss：0.4439\n",
      "[Epoch 15] Step 2550 / 3125 | Loss：0.4191\n",
      "[Epoch 15] Step 2600 / 3125 | Loss：0.4644\n",
      "[Epoch 15] Step 2650 / 3125 | Loss：0.4416\n",
      "[Epoch 15] Step 2700 / 3125 | Loss：0.4759\n",
      "[Epoch 15] Step 2750 / 3125 | Loss：0.4293\n",
      "[Epoch 15] Step 2800 / 3125 | Loss：0.5058\n",
      "[Epoch 15] Step 2850 / 3125 | Loss：0.4488\n",
      "[Epoch 15] Step 2900 / 3125 | Loss：0.4688\n",
      "[Epoch 15] Step 2950 / 3125 | Loss：0.4322\n",
      "[Epoch 15] Step 3000 / 3125 | Loss：0.4166\n",
      "[Epoch 15] Step 3050 / 3125 | Loss：0.4381\n",
      "[Epoch 15] Step 3100 / 3125 | Loss：0.4357\n",
      "Epoch 15 Result:\n",
      "  Train Loss: 0.4214\n",
      "  Val Loss:   0.4670\n",
      "  Val AUC:    0.7814\n",
      "\n",
      "Training Done! Total Time: 1303.2536790370941s\n"
     ]
    }
   ],
   "source": [
    "###训练循环###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import time\n",
    "\n",
    "# === 1. 准备工作 (Setup) ===\n",
    "# 检查是否有 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 实例化模型\n",
    "# 注意：sparse_feat_sizes 和 dense_feat_num 必须是你之前数据处理步骤算出来的变量\n",
    "model = DeepFM(sparse_feat_sizes, dense_feat_num, embedding_dim = 8, hidden_units = [256, 128, 64]).to(device)\n",
    "\n",
    "# 定义损失函数: 二分类交叉熵 (BCEWithLogitsLoss 自带 Sigmoid，数值稳定性更好)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 定义优化器: Adam (通常是推荐系统的首选)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-5) #告诉优化器所有可学习的参数以及设定学习率\n",
    "\n",
    "# === 2. 定义训练函数 (Train Engine) ===\n",
    "def train_one_epoch(model, train_loader, epoch_idx):\n",
    "    model.train() # 切换到训练模式 (启用 Dropout)\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, (dense_x, sparse_x, label) in enumerate(train_loader): #step会循环len(train_loader)次，也就是总批次数\n",
    "        # 1. 搬运数据到 GPU\n",
    "        dense_x, sparse_x, label = dense_x.to(device), sparse_x.to(device), label.to(device)\n",
    "\n",
    "        # 2. 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. 前向传播 (Forward)\n",
    "        outputs = model(dense_x, sparse_x)\n",
    "        \n",
    "        # 4. 计算 Loss\n",
    "        # 注意：outputs 是 [batch, 1]，label 是 [batch]，需要 view(-1, 1) 对齐维度 \n",
    "        # view实际上就是reshape，-1代表自动识别应该要多少行或者列才能保证逗号前后相乘等于原来的总共元素数\n",
    "        loss = criterion(outputs, label.view(-1, 1))\n",
    "\n",
    "        # 5. 反向传播 (Backward)\n",
    "        loss.backward()\n",
    "\n",
    "        # 6. 更新参数 (Update)\n",
    "        optimizer.step() \n",
    "        # loss.backward() 才是自动微分的核心（计算梯度）。\n",
    "        #它从计算得到的标量损失值 loss 开始，沿着网络向前传播的路径逆向追溯。\n",
    "        # optimizer.step() 只是负责“更新参数”（拿着算好的梯度去修改权重）。\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        '''oss 是一个 PyTorch 张量，即使它只包含一个标量值。\n",
    "        .item() 方法的作用是：将这个 PyTorch 张量中的标量值提取出来，\n",
    "        转换为一个标准的 Python 浮点数。'''\n",
    "\n",
    "        # 每 50 个 batch 打印一次进度\n",
    "        if (step + 1) % 50 ==0:\n",
    "            print(f\"[Epoch {epoch_idx}] Step {step + 1} / {len(train_loader)} | Loss：{loss.item():.4f}\")\n",
    "            \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# === 3. 定义验证函数 (Evaluation Engine) ===   \n",
    "def evaluate(model, val_loader):\n",
    "    model.eval() # 切换到评估模式 (关闭 Dropout)\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad(): # 验证时不需要计算梯度，省显存\n",
    "        for dense_x, sparse_x, label in val_loader:\n",
    "            dense_x, sparse_x, label = dense_x.to(device), sparse_x.to(device), label.to(device)\n",
    "\n",
    "            outputs = model(dense_x, sparse_x)\n",
    "\n",
    "            # 这里的 outputs 是 Logit，需要过 Sigmoid 变成概率 (0~1),推理阶段和训练阶段不同，不用更新参数，需要直观地利用sigmoid来反映概率，而不是没有直观概率意义的logit\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy() #numpy兼容cpu，而sklearn需要numpy格式为输入，这是python的东西\n",
    "            targets = label.cpu().numpy()\n",
    "\n",
    "            all_targets.extend(targets) #extend将可迭代对象的元素拆开来加入新的对象而不是append那种整个加入新对象\n",
    "            all_preds.extend(probs)\n",
    "            \n",
    "    # 计算核心指标\n",
    "    auc = roc_auc_score(all_targets, all_preds)\n",
    "    loss = log_loss(all_targets, all_preds) #L_i = - [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
    "    return auc, loss\n",
    "\n",
    "# === 4. 主循环 (Main Loop) ===\n",
    "EPOCHS = 15 # 训练 15 轮\n",
    "\n",
    "print(f\"Start Training for {EPOCHS} epochs...\")\n",
    "start_time = time.time() #返回当前的系统时间戳\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1} ---\")\n",
    "\n",
    "    # 1. 训练一轮\n",
    "    train_loss = train_one_epoch(model, train_loader, epoch + 1)\n",
    "\n",
    "    # 2. 验证效果\n",
    "    auc, val_loss = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Result:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  Val AUC:    {auc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining Done! Total Time: {time.time() - start_time}s\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f638eae-99ea-4237-99f6-e2573b8f5ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
